{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning an XGBoost Model\n",
    "\n",
    "The goal of this notebook is to train and evaluate an XGBoost model, comparing it's performance on a holdout set against other types of models (LR, SVC, LDA). \n",
    "\n",
    "To ensure reproducibility and consistent evaluation across models, all datasets were **pre-split into cross-val data and holdout data** as below:\n",
    "\n",
    "| Split type           | CV training file     | Holdout file              | Description                              |\n",
    "| -------------------- | -------------------- | ------------------------- | ---------------------------------------- |\n",
    "| **Random**           | `apps_cv_random.csv` | `apps_holdout_random.csv` | Simple random sampling                   |\n",
    "| **Stratified**       | `apps_cv_strat.csv`  | `apps_holdout_strat.csv`  | Stratified by `TARGET`                   |\n",
    "| **Multi-Stratified** | `apps_cv_multi.csv`  | `apps_holdout_multi.csv`  | Stratified by `TARGET` + `CODE_GENDER_M` |\n",
    "\n",
    "Each dataset for cross-validation (`apps_cv_*.csv`) also contains a column, `fold`, with pre-assigned folds from 1-5 using the corresponding splitting method to ensure consistent evaluation. Therefore, no additional splitting is needed inside this notebook -- can simply loop through assigned folds for cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from xgboost import XGBClassifier\n",
    "from itertools import product\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions\n",
    "\n",
    "Copying over metric functions from `cross_val.ipynb`. Note that in that file, we assigned folds already for each type of splitting to maintain consistent comparison across modeling. So, there will be no explicit splitting in this fild, we will just use the folds already created, but still make it clear what type of splitting was used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICS \n",
    "\n",
    "def classification_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    computes conf matrix + acc, prec, rec, and f1\n",
    "    \n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    # conf matrix\n",
    "    tp = np.sum((y_true==1) & (y_pred==1))\n",
    "    tn = np.sum((y_true==0) & (y_pred==0))\n",
    "    fp = np.sum((y_true==0) & (y_pred==1))\n",
    "    fn = np.sum((y_true==1) & (y_pred==0))\n",
    "\n",
    "    acc  = (tp + tn) / max((tp + tn + fp + fn), 1)\n",
    "    prec = tp / max((tp + fp), 1)\n",
    "    rec  = tp / max((tp + fn), 1)\n",
    "    f1   = (2*prec*rec / max((prec+rec), 1e-12)) if (prec+rec)>0 else 0.0\n",
    "\n",
    "    metrics = {\"n\":len(y_true),\n",
    "        \"tp\":tp, \"tn\": tn, \"fp\":fp, \"fn\":fn, \"acc\":acc, \"prec\":prec, \"rec\": rec, \"f1\":f1\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def roc_auc_from_probs(y_true, y_prob):\n",
    "    \n",
    "    desc_sort_indices = np.argsort(-y_prob)\n",
    "    y_true = np.array(y_true)[desc_sort_indices]\n",
    "    y_prob = np.array(y_prob)[desc_sort_indices]\n",
    "    pos = np.sum(y_true == 1)\n",
    "    neg = np.sum(y_true == 0)\n",
    "\n",
    "    # running totals for TPR/FPR\n",
    "    tpr = [0.0]\n",
    "    fpr = [0.0]\n",
    "    tp = fp = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "        tpr.append(tp / pos)\n",
    "        fpr.append(fp / neg)\n",
    "\n",
    "    # get auc\n",
    "    auc = np.trapz(tpr, fpr)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development\n",
    "\n",
    "**Notes:** \n",
    "- All evaluation will focus on stratified cross-validation, but we will test the other methods as well. \n",
    "- Recall that folds have been pre-assigned to ensure consistency across different model development processes\n",
    "- For our other models, we have decided to scale + PCA, but this is not necessary for nonlinear tree-based algorithms like XGBoost\n",
    "    - these can only really hurt XGBoost, so we will not use it here\n",
    "\n",
    "**Process:**\n",
    "1. Setting a baseline\n",
    "    - evaluating an xgb model with all default parameters to build off of\n",
    "2. Hyperparameter tuning\n",
    "    - evaluate many different combinations of parameters\n",
    "    - choose the best set based on average ROC-AUC across all folds\n",
    "3. Threshold tuning\n",
    "    - tweak the threshold on the best model to maximize F1 \n",
    "        - note that roc-auc is not affected by threshold, so we need a different optimizing metric\n",
    "4. Holdout evaluation\n",
    "    - evaluate on the corresponding holdout table. the performance here is what we will compare with other models (LR, SVC, LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "\n",
    "**Read in the data:**\n",
    "\n",
    "Recall that we have different datasets for each type of cross validation. We are currently focusing on the stratified splitting method, but random and multiple-stratification methods are avaiable for testing/comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps_cv_strat = pd.read_csv(\"data/apps_cv_strat.csv\")\n",
    "apps_holdout_strat = pd.read_csv(\"data/apps_holdout_strat.csv\")\n",
    "target_col = 'TARGET'\n",
    "feature_cols = [col for col in apps_cv_strat.columns if col not in [target_col, 'SK_ID_CURR', 'fold']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A cross-validation function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_xgb(data, feature_cols, target_col, params=None):\n",
    "    \n",
    "    if params == None:\n",
    "        params = {}\n",
    "\n",
    "    fold_metrics = []\n",
    "    for f in data.fold.unique():\n",
    "\n",
    "        # split into train and test based on folds\n",
    "        train = data[data.fold != f]\n",
    "        test = data[data.fold == f]\n",
    "        X_train, y_train = train[feature_cols], train[target_col]\n",
    "        X_test, y_test = test[feature_cols], test[target_col]\n",
    "\n",
    "        # fit model with specified params\n",
    "        model = XGBClassifier(eval_metric='auc', \n",
    "                              random_state=42, \n",
    "                              n_jobs=-1, \n",
    "                              tree_method='hist',\n",
    "                              **params\n",
    "                            )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # get predictions (probablities and decisions)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "        # calculate classification metrics from previously defined functions\n",
    "        metrics = classification_metrics(y_test, y_pred)\n",
    "        metrics['roc_auc'] = roc_auc_from_probs(y_test, y_prob)\n",
    "        metrics['fold'] = int(f)\n",
    "\n",
    "        # add to list of all fold metrics\n",
    "        fold_metrics.append(metrics)\n",
    "\n",
    "    # return results in dataframe\n",
    "    return pd.DataFrame(fold_metrics).sort_values(\"fold\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A grid search function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_xgb(data, feature_cols, target_col, param_grid):\n",
    "    \n",
    "    # get all possible combinations of parameters\n",
    "    keys = list(param_grid.keys())\n",
    "    combos = [dict(zip(keys, v)) for v in product(*param_grid.values())]\n",
    "\n",
    "    # initialize stuff for tracking and results\n",
    "    results = []\n",
    "    total = len(combos)\n",
    "    start = time.time()\n",
    "    next_checkpoint = 5 \n",
    "    best_roc_auc = 0\n",
    "    best_params = None\n",
    "\n",
    "    # evaluate every possible combo\n",
    "    for i, params in enumerate(combos, 1):\n",
    "\n",
    "        # run cross validation and store results\n",
    "        fold_results = cv_xgb(data, feature_cols, target_col, params)\n",
    "        mean_roc_auc = fold_results[\"roc_auc\"].mean()\n",
    "\n",
    "        results.append({\n",
    "            'params': params,\n",
    "            'mean_roc_auc': mean_roc_auc,\n",
    "            'mean_f1': fold_results['f1'].mean(),\n",
    "            'mean_acc': fold_results['acc'].mean(),\n",
    "            'mean_prec': fold_results['prec'].mean(),\n",
    "            'mean_rec': fold_results['rec'].mean(),\n",
    "        })\n",
    "\n",
    "        # tracker for updates\n",
    "        if  mean_roc_auc > best_roc_auc:\n",
    "            best_roc_auc =  mean_roc_auc\n",
    "            best_params = params\n",
    "\n",
    "        # print progress checkpoints\n",
    "        pct_done = (i/total)*100\n",
    "        elapsed = time.time() - start\n",
    "        if pct_done >= next_checkpoint or i == total:\n",
    "            print(f\"{i} / {total} ({pct_done:5.1f}% in {elapsed/60:.1f} mins) | Best ROC-AUC: {best_roc_auc:.4f} | Best Params: {best_params}\")\n",
    "            next_checkpoint += 5\n",
    "\n",
    "\n",
    "    \n",
    "    return pd.DataFrame(results).sort_values(\"mean_roc_auc\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting a Baseline\n",
    "\n",
    "Fitting an XGBoost model with default parameters to understand baseline predictive power and what we can build on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>acc</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49156</td>\n",
       "      <td>253</td>\n",
       "      <td>44913</td>\n",
       "      <td>274</td>\n",
       "      <td>3716</td>\n",
       "      <td>0.918830</td>\n",
       "      <td>0.480076</td>\n",
       "      <td>0.063744</td>\n",
       "      <td>0.112544</td>\n",
       "      <td>0.766700</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49156</td>\n",
       "      <td>224</td>\n",
       "      <td>44928</td>\n",
       "      <td>259</td>\n",
       "      <td>3745</td>\n",
       "      <td>0.918545</td>\n",
       "      <td>0.463768</td>\n",
       "      <td>0.056437</td>\n",
       "      <td>0.100629</td>\n",
       "      <td>0.766153</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49156</td>\n",
       "      <td>226</td>\n",
       "      <td>44935</td>\n",
       "      <td>252</td>\n",
       "      <td>3743</td>\n",
       "      <td>0.918728</td>\n",
       "      <td>0.472803</td>\n",
       "      <td>0.056941</td>\n",
       "      <td>0.101642</td>\n",
       "      <td>0.768592</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49155</td>\n",
       "      <td>232</td>\n",
       "      <td>44903</td>\n",
       "      <td>283</td>\n",
       "      <td>3737</td>\n",
       "      <td>0.918218</td>\n",
       "      <td>0.450485</td>\n",
       "      <td>0.058453</td>\n",
       "      <td>0.103479</td>\n",
       "      <td>0.772013</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49154</td>\n",
       "      <td>223</td>\n",
       "      <td>44899</td>\n",
       "      <td>287</td>\n",
       "      <td>3745</td>\n",
       "      <td>0.917972</td>\n",
       "      <td>0.437255</td>\n",
       "      <td>0.056200</td>\n",
       "      <td>0.099598</td>\n",
       "      <td>0.765584</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       n   tp     tn   fp    fn       acc      prec       rec        f1  \\\n",
       "0  49156  253  44913  274  3716  0.918830  0.480076  0.063744  0.112544   \n",
       "1  49156  224  44928  259  3745  0.918545  0.463768  0.056437  0.100629   \n",
       "2  49156  226  44935  252  3743  0.918728  0.472803  0.056941  0.101642   \n",
       "3  49155  232  44903  283  3737  0.918218  0.450485  0.058453  0.103479   \n",
       "4  49154  223  44899  287  3745  0.917972  0.437255  0.056200  0.099598   \n",
       "\n",
       "    roc_auc  fold  \n",
       "0  0.766700     1  \n",
       "1  0.766153     2  \n",
       "2  0.768592     3  \n",
       "3  0.772013     4  \n",
       "4  0.765584     5  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results = cv_xgb(apps_cv_strat, feature_cols, target_col, params=None)\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision, recall, and F1 are all quite low, while ROC-AUC is relatively strong. This indicates that the model is doing a good job ranking applicants from low-risk to high-risk, as in assigning higher probabilities to true positives, but the actual 0/1 decisions at the default threshold of 0.5 are poor (likely due to class imbalance).\n",
    "\n",
    "Therefore, we expect significant improvement after threshold tuning at the end. This is also why we focus on ROC-AUC during hyperparameter tuning -- as long as the modelâ€™s ability to rank cases is good (high ROC-AUC), we can later adjust the threshold to manipulate precision/recall/F1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Tuning\n",
    "\n",
    "Now, finding hyperparameters that maximize ROC-AUC (while keeping an eye on other metrics). \n",
    "\n",
    "XGBoost has a lot of parameters, many of which make a big impact on predictions, so the method of selecting the best ones is more complicated than the other linear models. A simple grid search over a huge parameter grid will take forever, considering just one cross-validated iteration of the baseline model took 15+ seconds. Therefore, a smarter approach may be to identify optimal ranges of certain parameters first, and then do a search over a narrower grid. So an iterative grid search instead of all at once. Our approach will likely adapt as we try things out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
