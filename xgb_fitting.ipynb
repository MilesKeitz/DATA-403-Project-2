{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning an XGBoost Model\n",
    "\n",
    "The goal of this notebook is to train and evaluate an XGBoost model, comparing it's performance on a holdout set against other types of models (LR, SVC, LDA). \n",
    "\n",
    "To ensure reproducibility and consistent evaluation across models, all datasets were **pre-split into cross-val data and holdout data** as below:\n",
    "\n",
    "| Split type           | CV training file     | Holdout file              | Description                              |\n",
    "| -------------------- | -------------------- | ------------------------- | ---------------------------------------- |\n",
    "| **Random**           | `apps_cv_random.csv` | `apps_holdout_random.csv` | Simple random sampling                   |\n",
    "| **Stratified**       | `apps_cv_strat.csv`  | `apps_holdout_strat.csv`  | Stratified by `TARGET`                   |\n",
    "| **Multi-Stratified** | `apps_cv_multi.csv`  | `apps_holdout_multi.csv`  | Stratified by `TARGET` + `CODE_GENDER_M` |\n",
    "\n",
    "Each dataset for cross-validation (`apps_cv_*.csv`) also contains a column, `fold`, with pre-assigned folds from 1-5 using the corresponding splitting method to ensure consistent evaluation. Therefore, no additional splitting is needed inside this notebook -- can simply loop through assigned folds for cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from xgboost import XGBClassifier\n",
    "from itertools import product\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions\n",
    "\n",
    "Copying over metric functions from `cross_val.ipynb`. Note that in that file, we assigned folds already for each type of splitting to maintain consistent comparison across modeling. So, there will be no explicit splitting in this fild, we will just use the folds already created, but still make it clear what type of splitting was used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICS \n",
    "\n",
    "def classification_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes confusion matrix + accuracy, precision, recall, F1, and balanced accuracy.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    # Confusion matrix components\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    # Metrics\n",
    "    acc  = (tp + tn) / max((tp + tn + fp + fn), 1)\n",
    "    prec = tp / max((tp + fp), 1)\n",
    "    rec  = tp / max((tp + fn), 1)\n",
    "    f1   = (2 * prec * rec / max((prec + rec), 1e-12)) if (prec + rec) > 0 else 0.0\n",
    "\n",
    "    # Specificity (True Negative Rate)\n",
    "    spec = tn / max((tn + fp), 1)\n",
    "\n",
    "    # Balanced accuracy\n",
    "    bal_acc = 0.5 * (rec + spec)\n",
    "\n",
    "    metrics = {\n",
    "        \"n\": len(y_true),\n",
    "        \"tp\": tp, \"tn\": tn, \"fp\": fp, \"fn\": fn,\n",
    "        \"acc\": acc, \"bal_acc\": bal_acc, \"prec\": prec, \"rec\": rec, \"spec\": spec,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def roc_auc_from_probs(y_true, y_prob):\n",
    "    \n",
    "    desc_sort_indices = np.argsort(-y_prob)\n",
    "    y_true = np.array(y_true)[desc_sort_indices]\n",
    "    y_prob = np.array(y_prob)[desc_sort_indices]\n",
    "    pos = np.sum(y_true == 1)\n",
    "    neg = np.sum(y_true == 0)\n",
    "\n",
    "    # running totals for TPR/FPR\n",
    "    tpr = [0.0]\n",
    "    fpr = [0.0]\n",
    "    tp = fp = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "        tpr.append(tp / pos)\n",
    "        fpr.append(fp / neg)\n",
    "\n",
    "    # get auc\n",
    "    auc = np.trapz(tpr, fpr)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development\n",
    "\n",
    "**Notes:** \n",
    "- All evaluation will focus on stratified cross-validation, but we will test the other methods as well. \n",
    "- Recall that folds have been pre-assigned to ensure consistency across different model development processes\n",
    "- For our other models, we have decided to scale + PCA, but this is not necessary for nonlinear tree-based algorithms like XGBoost\n",
    "    - these can only really hurt XGBoost, so we will not use it here\n",
    "\n",
    "**Process:**\n",
    "1. Setting a baseline\n",
    "    - evaluating an xgb model with all default parameters to build off of\n",
    "2. Hyperparameter tuning\n",
    "    - evaluate many different combinations of parameters\n",
    "    - choose the best set based on average ROC-AUC across all folds\n",
    "3. Threshold tuning\n",
    "    - tweak the threshold on the best model to maximize F1 \n",
    "        - note that roc-auc is not affected by threshold, so we need a different optimizing metric\n",
    "4. Holdout evaluation\n",
    "    - evaluate on the corresponding holdout table. the performance here is what we will compare with other models (LR, SVC, LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "\n",
    "### Read in the data:\n",
    "\n",
    "Recall that we have different datasets for each type of cross validation. We are currently focusing on the stratified splitting method, but random and multiple-stratification methods are avaiable for testing/comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps_cv_strat = pd.read_csv(\"data/apps_cv_strat.csv\")\n",
    "apps_holdout_strat = pd.read_csv(\"data/apps_holdout_strat.csv\")\n",
    "target_col = 'TARGET'\n",
    "feature_cols = [col for col in apps_cv_strat.columns if col not in \n",
    "                [target_col, 'SK_ID_CURR', 'fold', 'neighbors_target_mean_500']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we aren't doing PCA and xgboost handles correlated/unecessary features well, we can still simply our model a bit by removing some of them. It will help us run faster and assess feature importance at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 35 highly correlated features\n"
     ]
    }
   ],
   "source": [
    "corr = apps_cv_strat[feature_cols].corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "print(f\"Dropping {len(to_drop)} highly correlated features\")\n",
    "\n",
    "feature_cols_pruned = [f for f in feature_cols if f not in to_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A cross-validation function:\n",
    "\n",
    "Note that we will tune many xgboost parameters, but some we will set consistently for reproducability. These include:\n",
    "\n",
    "- eval_metric='auc' : tells xgboost to evaluate performance based on ROC-AUC\n",
    "- random_state=42 : due to random subsampling in tree building, we fix the random seed to get the same results every run\n",
    "- n_jobs=-1 : uses all available CPU cores in parallel to speed up training\n",
    "- tree_method='hist' : a histogram-based algorithm provided by xgboost that can be faster than the default exact method with similar performance\n",
    "- scale_pos_weight = neg/pos : corrects for class imbalance by upweighting minority class so the model focuses more on them in training. we may change this ratio throughout testing, but it won't be a part of grid searches\n",
    "\n",
    "These parameters will be constant during every training/evaluation iteration while other parameters are being tuned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_xgb(data, feature_cols, target_col, params=None):\n",
    "    \n",
    "    if params == None:\n",
    "        params = {}\n",
    "\n",
    "    fold_metrics = []\n",
    "    for f in data.fold.unique():\n",
    "\n",
    "        # split into train and test based on folds\n",
    "        train = data[data.fold != f]\n",
    "        test = data[data.fold == f]\n",
    "        X_train, y_train = train[feature_cols], train[target_col]\n",
    "        X_test, y_test = test[feature_cols], test[target_col]\n",
    "\n",
    "        # calculate counts for class weighting\n",
    "        pos = (y_train == 1).sum()\n",
    "        neg = (y_train == 0).sum()\n",
    "        balanced_weight = neg / max(pos, 1)\n",
    "\n",
    "        # fit model with specified params\n",
    "        model = XGBClassifier(eval_metric='auc', \n",
    "                              random_state=42, \n",
    "                              n_jobs=-1, \n",
    "                              tree_method='hist',\n",
    "                              scale_pos_weight=balanced_weight,\n",
    "                              **params\n",
    "                            )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # get predictions (probablities and decisions)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "        # calculate classification metrics from previously defined functions\n",
    "        metrics = classification_metrics(y_test, y_pred)\n",
    "        metrics['roc_auc'] = roc_auc_from_probs(y_test, y_prob)\n",
    "        metrics['fold'] = int(f)\n",
    "\n",
    "        # add to list of all fold metrics\n",
    "        fold_metrics.append(metrics)\n",
    "\n",
    "    # return results in dataframe\n",
    "    return pd.DataFrame(fold_metrics).sort_values(\"fold\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A grid search function:\n",
    "\n",
    "Tests every combination of hyperparameters -- very slow/inefficient, so consider size of grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_xgb(data, feature_cols, target_col, param_grid):\n",
    "    \n",
    "    # get all possible combinations of parameters\n",
    "    keys = list(param_grid.keys())\n",
    "    combos = [dict(zip(keys, v)) for v in product(*param_grid.values())]\n",
    "\n",
    "    # initialize stuff for tracking and results\n",
    "    results = []\n",
    "    total = len(combos)\n",
    "    start = time.time()\n",
    "    next_checkpoint = 5 \n",
    "    best_roc_auc = 0\n",
    "    best_params = None\n",
    "\n",
    "    # evaluate every possible combo\n",
    "    for i, params in enumerate(combos, 1):\n",
    "\n",
    "        # run cross validation and store results\n",
    "        fold_results = cv_xgb(data, feature_cols, target_col, params)\n",
    "        mean_roc_auc = fold_results[\"roc_auc\"].mean()\n",
    "\n",
    "        results.append({\n",
    "            'params': params,\n",
    "            'mean_roc_auc': mean_roc_auc,\n",
    "            'mean_f1': fold_results['f1'].mean(),\n",
    "            'mean_acc': fold_results['acc'].mean(),\n",
    "            'mean_bal_acc': fold_results['bal_acc'].mean(),\n",
    "            'mean_prec': fold_results['prec'].mean(),\n",
    "            'mean_rec': fold_results['rec'].mean(),\n",
    "        })\n",
    "\n",
    "        # tracker for updates\n",
    "        if  mean_roc_auc > best_roc_auc:\n",
    "            best_roc_auc =  mean_roc_auc\n",
    "            best_params = params\n",
    "\n",
    "        # print progress checkpoints\n",
    "        pct_done = (i/total)*100\n",
    "        elapsed = time.time() - start\n",
    "        if pct_done >= next_checkpoint or i == total:\n",
    "            print(f\"{i}/{total} ({pct_done:5.1f}% in {elapsed/60:.1f} mins) | Best ROC-AUC: {best_roc_auc:.4f} | Best Params: {best_params}\")\n",
    "            next_checkpoint += 5\n",
    "\n",
    "\n",
    "    \n",
    "    return pd.DataFrame(results).sort_values(\"mean_roc_auc\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting a Baseline\n",
    "\n",
    "Fitting an XGBoost model with default parameters to understand baseline predictive power and what we can build on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>acc</th>\n",
       "      <th>bal_acc</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>spec</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49156</td>\n",
       "      <td>2348</td>\n",
       "      <td>35419</td>\n",
       "      <td>9768</td>\n",
       "      <td>1621</td>\n",
       "      <td>0.768309</td>\n",
       "      <td>0.687708</td>\n",
       "      <td>0.193793</td>\n",
       "      <td>0.591585</td>\n",
       "      <td>0.783832</td>\n",
       "      <td>0.291949</td>\n",
       "      <td>0.763353</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49156</td>\n",
       "      <td>2362</td>\n",
       "      <td>35058</td>\n",
       "      <td>10129</td>\n",
       "      <td>1607</td>\n",
       "      <td>0.761250</td>\n",
       "      <td>0.685477</td>\n",
       "      <td>0.189096</td>\n",
       "      <td>0.595112</td>\n",
       "      <td>0.775843</td>\n",
       "      <td>0.286999</td>\n",
       "      <td>0.758464</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49156</td>\n",
       "      <td>2317</td>\n",
       "      <td>35387</td>\n",
       "      <td>9800</td>\n",
       "      <td>1652</td>\n",
       "      <td>0.767027</td>\n",
       "      <td>0.683449</td>\n",
       "      <td>0.191219</td>\n",
       "      <td>0.583774</td>\n",
       "      <td>0.783123</td>\n",
       "      <td>0.288077</td>\n",
       "      <td>0.759206</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49155</td>\n",
       "      <td>2416</td>\n",
       "      <td>35363</td>\n",
       "      <td>9823</td>\n",
       "      <td>1553</td>\n",
       "      <td>0.768569</td>\n",
       "      <td>0.695664</td>\n",
       "      <td>0.197402</td>\n",
       "      <td>0.608718</td>\n",
       "      <td>0.782610</td>\n",
       "      <td>0.298124</td>\n",
       "      <td>0.766919</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49154</td>\n",
       "      <td>2351</td>\n",
       "      <td>35477</td>\n",
       "      <td>9709</td>\n",
       "      <td>1617</td>\n",
       "      <td>0.769581</td>\n",
       "      <td>0.688811</td>\n",
       "      <td>0.194942</td>\n",
       "      <td>0.592490</td>\n",
       "      <td>0.785133</td>\n",
       "      <td>0.293362</td>\n",
       "      <td>0.763787</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       n    tp     tn     fp    fn       acc   bal_acc      prec       rec  \\\n",
       "0  49156  2348  35419   9768  1621  0.768309  0.687708  0.193793  0.591585   \n",
       "1  49156  2362  35058  10129  1607  0.761250  0.685477  0.189096  0.595112   \n",
       "2  49156  2317  35387   9800  1652  0.767027  0.683449  0.191219  0.583774   \n",
       "3  49155  2416  35363   9823  1553  0.768569  0.695664  0.197402  0.608718   \n",
       "4  49154  2351  35477   9709  1617  0.769581  0.688811  0.194942  0.592490   \n",
       "\n",
       "       spec        f1   roc_auc  fold  \n",
       "0  0.783832  0.291949  0.763353     1  \n",
       "1  0.775843  0.286999  0.758464     2  \n",
       "2  0.783123  0.288077  0.759206     3  \n",
       "3  0.782610  0.298124  0.766919     4  \n",
       "4  0.785133  0.293362  0.763787     5  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results = cv_xgb(apps_cv_strat, feature_cols, target_col, params=None) # send no parameters\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision, recall, and F1 are all quite low, while ROC-AUC is relatively strong. This indicates that the model is doing a good job ranking applicants from low-risk to high-risk, as in assigning higher probabilities to true positives, but the actual 0/1 decisions at the default threshold of 0.5 are poor (likely due to class imbalance).\n",
    "\n",
    "Therefore, we expect significant improvement after threshold tuning at the end. This is also why we focus on ROC-AUC during hyperparameter tuning -- as long as the model’s ability to rank cases is good (high ROC-AUC), we can later adjust the threshold to manipulate precision/recall/F1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Tuning\n",
    "\n",
    "Now, finding hyperparameters that maximize ROC-AUC (while keeping an eye on other metrics). \n",
    "\n",
    "XGBoost has a lot of parameters, many of which make a big impact on predictions, so the method of selecting the best ones is more complicated than the other linear models. A simple grid search over a huge parameter grid will take forever, considering just one cross-validated iteration of the baseline model took 15+ seconds. \n",
    "\n",
    "Therefore, a smart approach may be to do a multi-step grid search. There are two ways we could do this:\n",
    "\n",
    "1. Tree structure --> learning dynamics\n",
    "    - first a grid search on parameters that affect the shape of the trees essentially (max_depth, subsample, etc.)\n",
    "    - then a grid search on paramaters that affect how the model learns (learning_rate, estimators, etc.)\n",
    "2. Wide range, big step size --> small range, small step size\n",
    "    - a grid search on *all* parameters with a very wide range for each parameter\n",
    "    - then a more granular search on the best parameters to find exact optimal values\n",
    "\n",
    "We will start with the first structural approach, but then may incorporate the second approach at some points as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Pass:\n",
    "\n",
    "Optimizing tree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/225 (  5.3% in 7.5 mins) | Best ROC-AUC: 0.7790 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 3, 'min_child_weight': 3, 'subsample': 0.7, 'colsample_bytree': 0.85}\n",
      "23/225 ( 10.2% in 14.7 mins) | Best ROC-AUC: 0.7790 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 3, 'min_child_weight': 3, 'subsample': 0.7, 'colsample_bytree': 0.85}\n",
      "34/225 ( 15.1% in 21.5 mins) | Best ROC-AUC: 0.7791 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 3, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.85}\n",
      "45/225 ( 20.0% in 27.5 mins) | Best ROC-AUC: 0.7791 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 3, 'min_child_weight': 12, 'subsample': 0.7, 'colsample_bytree': 0.85}\n",
      "57/225 ( 25.3% in 34.6 mins) | Best ROC-AUC: 0.7810 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 3, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "68/225 ( 30.2% in 41.3 mins) | Best ROC-AUC: 0.7810 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.7, 'colsample_bytree': 0.85}\n",
      "79/225 ( 35.1% in 48.0 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "90/225 ( 40.0% in 55.1 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "102/225 ( 45.3% in 65.0 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "113/225 ( 50.2% in 73.0 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "124/225 ( 55.1% in 80.9 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "135/225 ( 60.0% in 88.5 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "147/225 ( 65.3% in 97.7 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "158/225 ( 70.2% in 106.1 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "169/225 ( 75.1% in 114.5 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "180/225 ( 80.0% in 122.9 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "192/225 ( 85.3% in 133.7 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "203/225 ( 90.2% in 143.3 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "214/225 ( 95.1% in 152.8 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "225/225 (100.0% in 162.2 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "\n",
    "    # fix learning dynamics this pass\n",
    "    \"learning_rate\": [0.05], # low learning rate at first to establish reliable tree structure\n",
    "    \"n_estimators\": [500], \n",
    "\n",
    "    # tree structure parameters to test\n",
    "    \"max_depth\": [3, 4, 5, 6, 7],\n",
    "    \"min_child_weight\": [1, 3, 5, 8, 12],\n",
    "    \"subsample\": [0.7, 0.85, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 0.85, 1.0],\n",
    "}\n",
    "\n",
    "search1_results = grid_search_xgb(apps_cv_strat, feature_cols, target_col, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save so we dont have to run it all again\n",
    "search1_results.to_csv('results/xgb_search1_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_roc_auc</th>\n",
       "      <th>mean_prec</th>\n",
       "      <th>mean_rec</th>\n",
       "      <th>mean_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 500, '...</td>\n",
       "      <td>0.781358</td>\n",
       "      <td>0.184567</td>\n",
       "      <td>0.685850</td>\n",
       "      <td>0.290855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 500, '...</td>\n",
       "      <td>0.781235</td>\n",
       "      <td>0.192193</td>\n",
       "      <td>0.661913</td>\n",
       "      <td>0.297880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 500, '...</td>\n",
       "      <td>0.781106</td>\n",
       "      <td>0.184413</td>\n",
       "      <td>0.684237</td>\n",
       "      <td>0.290517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 500, '...</td>\n",
       "      <td>0.781089</td>\n",
       "      <td>0.184827</td>\n",
       "      <td>0.684439</td>\n",
       "      <td>0.291051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 500, '...</td>\n",
       "      <td>0.781088</td>\n",
       "      <td>0.192360</td>\n",
       "      <td>0.661510</td>\n",
       "      <td>0.298040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 500, '...</td>\n",
       "      <td>0.781088</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>0.684943</td>\n",
       "      <td>0.290832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 500, '...</td>\n",
       "      <td>0.781080</td>\n",
       "      <td>0.184300</td>\n",
       "      <td>0.686051</td>\n",
       "      <td>0.290543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 500, '...</td>\n",
       "      <td>0.781050</td>\n",
       "      <td>0.191841</td>\n",
       "      <td>0.664533</td>\n",
       "      <td>0.297723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 500, '...</td>\n",
       "      <td>0.781029</td>\n",
       "      <td>0.184236</td>\n",
       "      <td>0.683632</td>\n",
       "      <td>0.290245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 500, '...</td>\n",
       "      <td>0.781008</td>\n",
       "      <td>0.184761</td>\n",
       "      <td>0.684539</td>\n",
       "      <td>0.290977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              params  mean_roc_auc  mean_prec  \\\n",
       "0  {'learning_rate': 0.05, 'n_estimators': 500, '...      0.781358   0.184567   \n",
       "1  {'learning_rate': 0.05, 'n_estimators': 500, '...      0.781235   0.192193   \n",
       "2  {'learning_rate': 0.05, 'n_estimators': 500, '...      0.781106   0.184413   \n",
       "3  {'learning_rate': 0.05, 'n_estimators': 500, '...      0.781089   0.184827   \n",
       "4  {'learning_rate': 0.05, 'n_estimators': 500, '...      0.781088   0.192360   \n",
       "5  {'learning_rate': 0.05, 'n_estimators': 500, '...      0.781088   0.184615   \n",
       "6  {'learning_rate': 0.05, 'n_estimators': 500, '...      0.781080   0.184300   \n",
       "7  {'learning_rate': 0.05, 'n_estimators': 500, '...      0.781050   0.191841   \n",
       "8  {'learning_rate': 0.05, 'n_estimators': 500, '...      0.781029   0.184236   \n",
       "9  {'learning_rate': 0.05, 'n_estimators': 500, '...      0.781008   0.184761   \n",
       "\n",
       "   mean_rec   mean_f1  \n",
       "0  0.685850  0.290855  \n",
       "1  0.661913  0.297880  \n",
       "2  0.684237  0.290517  \n",
       "3  0.684439  0.291051  \n",
       "4  0.661510  0.298040  \n",
       "5  0.684943  0.290832  \n",
       "6  0.686051  0.290543  \n",
       "7  0.664533  0.297723  \n",
       "8  0.683632  0.290245  \n",
       "9  0.684539  0.290977  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search1_results.sort_values(by='mean_roc_auc', ascending=False)[\n",
    "    ['params', 'mean_roc_auc', 'mean_prec', 'mean_rec', 'mean_f1']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we found decent lift (+0.02 ROC-AUC) from tuning the tree structure. We also virtually identical scores at the top, not just one outlier, which is a good sign we found an optimal structure -- if these top scores had similar parameters, that would be further proof:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_roc_auc</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>subsample</th>\n",
       "      <th>colsample_bytree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.781358</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.781235</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.781106</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.781089</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.781088</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_roc_auc  max_depth  min_child_weight  subsample  colsample_bytree\n",
       "0      0.781358        4.0               8.0       0.70               0.7\n",
       "1      0.781235        5.0               8.0       0.85               1.0\n",
       "2      0.781106        4.0              12.0       0.70               1.0\n",
       "3      0.781089        4.0              12.0       0.85               1.0\n",
       "4      0.781088        5.0               8.0       0.70               1.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_df = search1_results['params'].apply(pd.Series)\n",
    "results = pd.concat([search1_results.drop(columns='params'), params_df], axis=1)\n",
    "results.sort_values(by='mean_roc_auc', ascending=False)[\n",
    "    ['mean_roc_auc', 'max_depth', 'min_child_weight', 'subsample', 'colsample_bytree']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 5 models by ROC-AUC have very small changes in parameters -- The optimal region occurs with moderate `depth` (4–5), high `min_child_weight` (8–12), and subsampling between 0.7–0.85, suggesting strong generalization and reproducability. We also don't see any sitting on a single edge, like if all the best performers were at the highest depth, so there is no reason to believe we are missing anything beyond our grid. \n",
    "\n",
    "These parameters will be fixed while tuning learning dynamics in the next phase:\n",
    "\n",
    "- max_depth = 4\n",
    "- min_child_weight = 8\n",
    "- subsample = 0.7\n",
    "- colsample_bytree = 1\n",
    "\n",
    "These were the parameters of the best model by ROC-AUC, *except* for `colsample_bytree` because four out of the top five models had this parameter set to 1, so I felt it was safest to keep it as that, even though the #1 model was 0.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Pass: Learning Dynamics\n",
    "\n",
    "Now that we have a reliable tree structure, we will focus on the parameters that effect how the model learns. \n",
    "\n",
    "I think in this case it makes more sense to do the wide-then-narrow search approach because the learning parameters are continuous and small changes can make a big difference. Therefore, we will first find the appropriate scale of each parameter, and then do a narrow search over a smaller range to find more exact optimal parameters. Also, the difference from the best to 2nd (or even 5th) best is so small it's negligble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding Scale of Learning Dynamics:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "\n",
    "    # fix tree structure to optimal parameters found in pass 1\n",
    "    \"max_depth\": [4],\n",
    "    \"min_child_weight\": [8],\n",
    "    \"subsample\": [0.7],\n",
    "    \"colsample_bytree\": [1],\n",
    "\n",
    "    # wide grid to find scale of learning dynamics\n",
    "    \"learning_rate\": [0.01, 0.03, 0.05, 0.07, 0.10],\n",
    "    \"n_estimators\": [300, 500, 800, 1200],\n",
    "    \"gamma\": [0.0, 0.1, 0.2],\n",
    "    \"reg_alpha\": [0.0, 0.5, 1.0],\n",
    "    \"reg_lambda\": [1.0, 2.0, 5.0],\n",
    "}\n",
    "\n",
    "search2_results = grid_search_xgb(apps_cv_strat, feature_cols, target_col, param_grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
