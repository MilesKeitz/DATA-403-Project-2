{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning an XGBoost Model\n",
    "\n",
    "The goal of this notebook is to train and evaluate an XGBoost model, comparing it's performance on a holdout set against other types of models (LR, SVC, LDA). \n",
    "\n",
    "To ensure reproducibility and consistent evaluation across models, all datasets were **pre-split into cross-val data and holdout data** as below:\n",
    "\n",
    "| Split type           | CV training file     | Holdout file              | Description                              |\n",
    "| -------------------- | -------------------- | ------------------------- | ---------------------------------------- |\n",
    "| **Random**           | `apps_cv_random.csv` | `apps_holdout_random.csv` | Simple random sampling                   |\n",
    "| **Stratified**       | `apps_cv_strat.csv`  | `apps_holdout_strat.csv`  | Stratified by `TARGET`                   |\n",
    "| **Multi-Stratified** | `apps_cv_multi.csv`  | `apps_holdout_multi.csv`  | Stratified by `TARGET` + `CODE_GENDER_M` |\n",
    "\n",
    "Each dataset for cross-validation (`apps_cv_*.csv`) also contains a column, `fold`, with pre-assigned folds from 1-5 using the corresponding splitting method to ensure consistent evaluation. Therefore, no additional splitting is needed inside this notebook -- can simply loop through assigned folds for cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from xgboost import XGBClassifier\n",
    "from itertools import product\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions\n",
    "\n",
    "#### Metric calculators:\n",
    "\n",
    "Copied from `cross_val.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICS \n",
    "\n",
    "def classification_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes confusion matrix + accuracy, precision, recall, F1, and balanced accuracy.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    # Confusion matrix components\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    # Metrics\n",
    "    acc  = (tp + tn) / max((tp + tn + fp + fn), 1)\n",
    "    prec = tp / max((tp + fp), 1)\n",
    "    rec  = tp / max((tp + fn), 1)\n",
    "    f1   = (2 * prec * rec / max((prec + rec), 1e-12)) if (prec + rec) > 0 else 0.0\n",
    "\n",
    "    # Specificity (True Negative Rate)\n",
    "    spec = tn / max((tn + fp), 1)\n",
    "\n",
    "    # Balanced accuracy\n",
    "    bal_acc = 0.5 * (rec + spec)\n",
    "\n",
    "    metrics = {\n",
    "        \"n\": len(y_true),\n",
    "        \"tp\": tp, \"tn\": tn, \"fp\": fp, \"fn\": fn,\n",
    "        \"acc\": acc, \"bal_acc\": bal_acc, \"prec\": prec, \"rec\": rec, \"spec\": spec,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def roc_auc_from_probs(y_true, y_prob):\n",
    "    \n",
    "    desc_sort_indices = np.argsort(-y_prob)\n",
    "    y_true = np.array(y_true)[desc_sort_indices]\n",
    "    y_prob = np.array(y_prob)[desc_sort_indices]\n",
    "    pos = np.sum(y_true == 1)\n",
    "    neg = np.sum(y_true == 0)\n",
    "\n",
    "    # running totals for TPR/FPR\n",
    "    tpr = [0.0]\n",
    "    fpr = [0.0]\n",
    "    tp = fp = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "        tpr.append(tp / pos)\n",
    "        fpr.append(fp / neg)\n",
    "\n",
    "    # get auc\n",
    "    auc = np.trapz(tpr, fpr)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation function\n",
    "\n",
    "Note that in `cross_val.ipynb`, we assigned folds already for each type of splitting to maintain consistent comparison across modeling. So, there will be no explicit splitting in this file, we will just use the folds already created according to the type of split we want to use.\n",
    "\n",
    "Also, while we will be tuning many xgboost parameters, some we will be set consistently for reproducability. These include:\n",
    "\n",
    "- eval_metric='auc' : tells xgboost to evaluate performance based on ROC-AUC\n",
    "- random_state=42 : due to random subsampling in tree building, we fix the random seed to get the same results every run\n",
    "- n_jobs=-1 : uses all available CPU cores in parallel to speed up training\n",
    "- tree_method='hist' : a histogram-based algorithm provided by xgboost that can be faster than the default exact method with similar performance\n",
    "- scale_pos_weight = neg/pos : corrects for class imbalance by upweighting minority class so the model focuses more on them in training. we may change this ratio throughout testing, but it won't be a part of grid searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_xgb(data, feature_cols, target_col, params=None):\n",
    "    \n",
    "    if params == None:\n",
    "        params = {}\n",
    "\n",
    "    fold_metrics = []\n",
    "    fold_preds = []\n",
    "    for f in data.fold.unique():\n",
    "\n",
    "        # split into train and test based on folds\n",
    "        train = data[data.fold != f]\n",
    "        test = data[data.fold == f]\n",
    "        X_train, y_train = train[feature_cols], train[target_col]\n",
    "        X_test, y_test = test[feature_cols], test[target_col]\n",
    "\n",
    "        # calculate counts for class weighting\n",
    "        pos = (y_train == 1).sum()\n",
    "        neg = (y_train == 0).sum()\n",
    "        balanced_weight = (neg / max(pos, 1)) * 0.5\n",
    "\n",
    "        # fit model with specified params\n",
    "        model = XGBClassifier(eval_metric='auc', \n",
    "                              random_state=42, \n",
    "                              n_jobs=-1, \n",
    "                              tree_method='hist',\n",
    "                              scale_pos_weight=balanced_weight,\n",
    "                              **params\n",
    "                            )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # get predictions (probablities and decisions)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        y_train_prob = model.predict_proba(X_train)[:,1] # make training predictions as well to assess overfit\n",
    "        y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "        # calculate classification metrics from previously defined functions\n",
    "        metrics = classification_metrics(y_test, y_pred)\n",
    "        metrics['roc_auc'] = roc_auc_from_probs(y_test, y_prob)\n",
    "        metrics['train_roc_auc'] = roc_auc_from_probs(y_train, y_train_prob)\n",
    "        metrics['fold'] = int(f)\n",
    "\n",
    "        # add to list of all fold metrics\n",
    "        fold_metrics.append(metrics)\n",
    "\n",
    "        # store raw fold predictions for later threshold tuning\n",
    "        fold_preds.append(pd.DataFrame({\n",
    "            \"fold\": f,\n",
    "            \"y_true\": y_test.values,\n",
    "            \"y_prob\": y_prob\n",
    "        }))\n",
    "\n",
    "    # combine fold predictions into one long DataFrame\n",
    "    preds_df = pd.concat(fold_preds, ignore_index=True)\n",
    "\n",
    "    # summary DataFrame for quick metrics view\n",
    "    metrics_df = pd.DataFrame(fold_metrics).sort_values(\"fold\").reset_index(drop=True)\n",
    "\n",
    "    # return results in dataframe\n",
    "    return metrics_df, preds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A grid-search function:\n",
    "\n",
    "Tests every combination of hyperparameters -- very slow/inefficient, so consider size of grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_xgb(data, feature_cols, target_col, param_grid):\n",
    "    \n",
    "    # get all possible combinations of parameters\n",
    "    keys = list(param_grid.keys())\n",
    "    combos = [dict(zip(keys, v)) for v in product(*param_grid.values())]\n",
    "\n",
    "    # initialize stuff for tracking and results\n",
    "    results = []\n",
    "    total = len(combos)\n",
    "    start = time.time()\n",
    "    next_checkpoint = 5 \n",
    "    best_roc_auc = 0\n",
    "    best_params = None\n",
    "\n",
    "    # evaluate every possible combo\n",
    "    for i, params in enumerate(combos, 1):\n",
    "\n",
    "        # run cross validation and store results\n",
    "        metrics, preds = cv_xgb(data, feature_cols, target_col, params)\n",
    "        mean_roc_auc = metrics[\"roc_auc\"].mean()\n",
    "\n",
    "        results.append({\n",
    "            'params': params,\n",
    "            'mean_roc_auc': mean_roc_auc,\n",
    "            'mean_f1': metrics['f1'].mean(),\n",
    "            'mean_acc': metrics['acc'].mean(),\n",
    "            'mean_bal_acc': metrics['bal_acc'].mean(),\n",
    "            'mean_prec': metrics['prec'].mean(),\n",
    "            'mean_rec': metrics['rec'].mean(),\n",
    "        })\n",
    "\n",
    "        # tracker for updates\n",
    "        if  mean_roc_auc > best_roc_auc:\n",
    "            best_roc_auc =  mean_roc_auc\n",
    "            best_params = params\n",
    "\n",
    "        # print progress checkpoints\n",
    "        pct_done = (i/total)*100\n",
    "        elapsed = time.time() - start\n",
    "        if pct_done >= next_checkpoint or i == total:\n",
    "            print(f\"{i}/{total} ({pct_done:5.1f}% in {elapsed/60:.1f} mins) | Best ROC-AUC: {best_roc_auc:.4f} | Best Params: {best_params}\")\n",
    "            next_checkpoint += 5\n",
    "\n",
    "\n",
    "    \n",
    "    return pd.DataFrame(results).sort_values(\"mean_roc_auc\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development\n",
    "\n",
    "**Notes:** \n",
    "- All evaluation will focus on stratified cross-validation, but we will test the other methods as well. \n",
    "- Recall that folds have been pre-assigned to ensure consistency across different model development processes\n",
    "- For our other models, we have decided to scale + PCA, but this is not necessary for nonlinear tree-based algorithms like XGBoost\n",
    "    - these can only really hurt XGBoost, so we will not use it here\n",
    "\n",
    "**Process:**\n",
    "1. Setting a baseline\n",
    "    - evaluating an xgb model with all default parameters to build off of\n",
    "2. Hyperparameter tuning\n",
    "    - evaluate many different combinations of parameters\n",
    "    - choose the best set based on average ROC-AUC across all folds\n",
    "3. Holdout evaluation\n",
    "    - evaluate on the corresponding holdout table. the performance here is what we will compare with other models (LR, SVC, LDA)\n",
    "4. Threshold tuning\n",
    "    - tweak the threshold on the best model to maximize another chosen metric (recall, precision, f1, balanced accuracy, etc.) \n",
    "        - note that roc-auc is not affected by threshold, hence the need a different optimizing metric\n",
    "    - what metric we choose to optimize with threshold depends on business needs\n",
    "        - consider the cost of mislabeling someone as high risk? or trusting an applicant that you shouldn't? will there be human review?\n",
    "        - something we can include in the right up as optionality moving forward, not something we have to decide now on our own\n",
    "        - \"our model is very solid at ranking applicants from low-risk to high-risk, but in terms of actual classification, we can move the threshold based on what matters most to the business\"\n",
    "5. Fairness evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "Recall that we have different datasets for each type of cross validation. We are currently focusing on the stratified splitting method, but random and multiple-stratification methods are avaiable for testing/comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps_cv_strat = pd.read_csv(\"data/apps_cv_strat.csv\")\n",
    "apps_holdout_strat = pd.read_csv(\"data/apps_holdout_strat.csv\")\n",
    "target_col = 'TARGET'\n",
    "feature_cols = [col for col in apps_cv_strat.columns if col not in \n",
    "                [target_col, 'SK_ID_CURR', 'fold', 'neighbors_target_mean_500', 'AGE_INT', 'CODE_GENDER_M',\n",
    "                 'CODE_GENDER_XNA', 'DAYS_BIRTH',\n",
    "                 'NAME_FAMILY_STATUS_Previously Married', 'NAME_FAMILY_STATUS_Single']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we aren't doing PCA and xgboost handles correlated/unecessary features well, we can still simply our model a bit by removing some of them. It may help us run faster and assess feature importance at the end. This may or may not be used depending on results, but I'll make the set now to keep as an option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 35 highly correlated features\n"
     ]
    }
   ],
   "source": [
    "corr = apps_cv_strat[feature_cols].corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "print(f\"Dropping {len(to_drop)} highly correlated features\")\n",
    "\n",
    "feature_cols_pruned = [f for f in feature_cols if f not in to_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting a Baseline\n",
    "\n",
    "Fitting an XGBoost model with default parameters to understand baseline predictive power and what we can build on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>acc</th>\n",
       "      <th>bal_acc</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>spec</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49156</td>\n",
       "      <td>1603</td>\n",
       "      <td>40700</td>\n",
       "      <td>4487</td>\n",
       "      <td>2366</td>\n",
       "      <td>0.860587</td>\n",
       "      <td>0.652291</td>\n",
       "      <td>0.263218</td>\n",
       "      <td>0.403880</td>\n",
       "      <td>0.900702</td>\n",
       "      <td>0.318720</td>\n",
       "      <td>0.767795</td>\n",
       "      <td>0.906028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49156</td>\n",
       "      <td>1621</td>\n",
       "      <td>40578</td>\n",
       "      <td>4609</td>\n",
       "      <td>2348</td>\n",
       "      <td>0.858471</td>\n",
       "      <td>0.653208</td>\n",
       "      <td>0.260193</td>\n",
       "      <td>0.408415</td>\n",
       "      <td>0.898002</td>\n",
       "      <td>0.317874</td>\n",
       "      <td>0.766144</td>\n",
       "      <td>0.903025</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49156</td>\n",
       "      <td>1523</td>\n",
       "      <td>40422</td>\n",
       "      <td>4765</td>\n",
       "      <td>2446</td>\n",
       "      <td>0.853304</td>\n",
       "      <td>0.639137</td>\n",
       "      <td>0.242207</td>\n",
       "      <td>0.383724</td>\n",
       "      <td>0.894549</td>\n",
       "      <td>0.296968</td>\n",
       "      <td>0.749658</td>\n",
       "      <td>0.903107</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49155</td>\n",
       "      <td>1563</td>\n",
       "      <td>40575</td>\n",
       "      <td>4611</td>\n",
       "      <td>2406</td>\n",
       "      <td>0.857247</td>\n",
       "      <td>0.645879</td>\n",
       "      <td>0.253158</td>\n",
       "      <td>0.393802</td>\n",
       "      <td>0.897955</td>\n",
       "      <td>0.308193</td>\n",
       "      <td>0.762121</td>\n",
       "      <td>0.905266</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49154</td>\n",
       "      <td>1597</td>\n",
       "      <td>40603</td>\n",
       "      <td>4583</td>\n",
       "      <td>2371</td>\n",
       "      <td>0.858526</td>\n",
       "      <td>0.650522</td>\n",
       "      <td>0.258414</td>\n",
       "      <td>0.402470</td>\n",
       "      <td>0.898575</td>\n",
       "      <td>0.314742</td>\n",
       "      <td>0.759541</td>\n",
       "      <td>0.904998</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       n    tp     tn    fp    fn       acc   bal_acc      prec       rec  \\\n",
       "0  49156  1603  40700  4487  2366  0.860587  0.652291  0.263218  0.403880   \n",
       "1  49156  1621  40578  4609  2348  0.858471  0.653208  0.260193  0.408415   \n",
       "2  49156  1523  40422  4765  2446  0.853304  0.639137  0.242207  0.383724   \n",
       "3  49155  1563  40575  4611  2406  0.857247  0.645879  0.253158  0.393802   \n",
       "4  49154  1597  40603  4583  2371  0.858526  0.650522  0.258414  0.402470   \n",
       "\n",
       "       spec        f1   roc_auc  train_roc_auc  fold  \n",
       "0  0.900702  0.318720  0.767795       0.906028     1  \n",
       "1  0.898002  0.317874  0.766144       0.903025     2  \n",
       "2  0.894549  0.296968  0.749658       0.903107     3  \n",
       "3  0.897955  0.308193  0.762121       0.905266     4  \n",
       "4  0.898575  0.314742  0.759541       0.904998     5  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_metrics, baseline_preds = cv_xgb(apps_cv_strat, feature_cols, target_col, params=None) # send no parameters\n",
    "baseline_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision, recall, and F1 are all quite low, while ROC-AUC is relatively strong. This indicates that the model is doing a good job ranking applicants from low-risk to high-risk, as in assigning higher probabilities to true positives, but the actual 0/1 decisions at the default threshold of 0.5 are poor (likely due to class imbalance). Also, the training ROC-AUC is even higher than the baseline ROC-AUC indicating some overfit risk. \n",
    "\n",
    "Therefore, we expect significant improvement after threshold tuning at the end. This is also why we focus on ROC-AUC during hyperparameter tuning -- as long as the model’s ability to rank cases is good (high ROC-AUC), we can later adjust the threshold to manipulate precision/recall/F1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Tuning\n",
    "\n",
    "Now, finding hyperparameters that maximize ROC-AUC (while keeping an eye on other metrics). \n",
    "\n",
    "XGBoost has a lot of parameters, many of which make a big impact on predictions, so the method of selecting the best ones is more complicated than the other linear models. A simple grid search over a huge parameter grid will take forever, considering just one cross-validated iteration of the baseline model took 15+ seconds. \n",
    "\n",
    "Therefore, a smart approach may be to do a multi-step grid search. There are two ways we could do this:\n",
    "\n",
    "1. Tree structure --> learning dynamics\n",
    "    - first a grid search on parameters that affect the shape of the trees essentially (max_depth, subsample, etc.)\n",
    "    - then a grid search on paramaters that affect how the model learns (learning_rate, estimators, etc.)\n",
    "2. Wide range, big step size --> small range, small step size\n",
    "    - a grid search on *all* parameters with a very wide range for each parameter\n",
    "    - then a more granular search on the best parameters to find exact optimal values\n",
    "\n",
    "We will start with the first structural approach, but then may incorporate the second approach at some points as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Pass:\n",
    "\n",
    "Optimizing tree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/225 (  5.3% in 7.5 mins) | Best ROC-AUC: 0.7790 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 3, 'min_child_weight': 3, 'subsample': 0.7, 'colsample_bytree': 0.85}\n",
      "23/225 ( 10.2% in 14.7 mins) | Best ROC-AUC: 0.7790 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 3, 'min_child_weight': 3, 'subsample': 0.7, 'colsample_bytree': 0.85}\n",
      "34/225 ( 15.1% in 21.5 mins) | Best ROC-AUC: 0.7791 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 3, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.85}\n",
      "45/225 ( 20.0% in 27.5 mins) | Best ROC-AUC: 0.7791 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 3, 'min_child_weight': 12, 'subsample': 0.7, 'colsample_bytree': 0.85}\n",
      "57/225 ( 25.3% in 34.6 mins) | Best ROC-AUC: 0.7810 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 3, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "68/225 ( 30.2% in 41.3 mins) | Best ROC-AUC: 0.7810 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.7, 'colsample_bytree': 0.85}\n",
      "79/225 ( 35.1% in 48.0 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "90/225 ( 40.0% in 55.1 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "102/225 ( 45.3% in 65.0 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "113/225 ( 50.2% in 73.0 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "124/225 ( 55.1% in 80.9 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "135/225 ( 60.0% in 88.5 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "147/225 ( 65.3% in 97.7 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "158/225 ( 70.2% in 106.1 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "169/225 ( 75.1% in 114.5 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "180/225 ( 80.0% in 122.9 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "192/225 ( 85.3% in 133.7 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "203/225 ( 90.2% in 143.3 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "214/225 ( 95.1% in 152.8 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n",
      "225/225 (100.0% in 162.2 mins) | Best ROC-AUC: 0.7814 | Best Params: {'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 0.7}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "\n",
    "    # fix learning dynamics this pass\n",
    "    \"learning_rate\": [0.05], # low learning rate at first to establish reliable tree structure\n",
    "    \"n_estimators\": [500], \n",
    "\n",
    "    # tree structure parameters to test\n",
    "    \"max_depth\": [3, 4, 5, 6, 7],\n",
    "    \"min_child_weight\": [1, 3, 5, 8, 12],\n",
    "    \"subsample\": [0.7, 0.85, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 0.85, 1.0]\n",
    "}\n",
    "\n",
    "search1_results = grid_search_xgb(apps_cv_strat, feature_cols, target_col, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save so we dont have to run it all again\n",
    "search1_results.to_csv('results/xgb_search1_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_roc_auc</th>\n",
       "      <th>mean_prec</th>\n",
       "      <th>mean_rec</th>\n",
       "      <th>mean_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 500, '...</td>\n",
       "      <td>0.781358</td>\n",
       "      <td>0.184567</td>\n",
       "      <td>0.685850</td>\n",
       "      <td>0.290855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 500, '...</td>\n",
       "      <td>0.781235</td>\n",
       "      <td>0.192193</td>\n",
       "      <td>0.661913</td>\n",
       "      <td>0.297880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 500, '...</td>\n",
       "      <td>0.781106</td>\n",
       "      <td>0.184413</td>\n",
       "      <td>0.684237</td>\n",
       "      <td>0.290517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 500, '...</td>\n",
       "      <td>0.781089</td>\n",
       "      <td>0.184827</td>\n",
       "      <td>0.684439</td>\n",
       "      <td>0.291051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 500, '...</td>\n",
       "      <td>0.781088</td>\n",
       "      <td>0.192360</td>\n",
       "      <td>0.661510</td>\n",
       "      <td>0.298040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 500, '...</td>\n",
       "      <td>0.781088</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>0.684943</td>\n",
       "      <td>0.290832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 500, '...</td>\n",
       "      <td>0.781080</td>\n",
       "      <td>0.184300</td>\n",
       "      <td>0.686051</td>\n",
       "      <td>0.290543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 500, '...</td>\n",
       "      <td>0.781050</td>\n",
       "      <td>0.191841</td>\n",
       "      <td>0.664533</td>\n",
       "      <td>0.297723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 500, '...</td>\n",
       "      <td>0.781029</td>\n",
       "      <td>0.184236</td>\n",
       "      <td>0.683632</td>\n",
       "      <td>0.290245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 500, '...</td>\n",
       "      <td>0.781008</td>\n",
       "      <td>0.184761</td>\n",
       "      <td>0.684539</td>\n",
       "      <td>0.290977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              params  mean_roc_auc  mean_prec  \\\n",
       "0  {'learning_rate': 0.05, 'n_estimators': 500, '...      0.781358   0.184567   \n",
       "1  {'learning_rate': 0.05, 'n_estimators': 500, '...      0.781235   0.192193   \n",
       "2  {'learning_rate': 0.05, 'n_estimators': 500, '...      0.781106   0.184413   \n",
       "3  {'learning_rate': 0.05, 'n_estimators': 500, '...      0.781089   0.184827   \n",
       "4  {'learning_rate': 0.05, 'n_estimators': 500, '...      0.781088   0.192360   \n",
       "5  {'learning_rate': 0.05, 'n_estimators': 500, '...      0.781088   0.184615   \n",
       "6  {'learning_rate': 0.05, 'n_estimators': 500, '...      0.781080   0.184300   \n",
       "7  {'learning_rate': 0.05, 'n_estimators': 500, '...      0.781050   0.191841   \n",
       "8  {'learning_rate': 0.05, 'n_estimators': 500, '...      0.781029   0.184236   \n",
       "9  {'learning_rate': 0.05, 'n_estimators': 500, '...      0.781008   0.184761   \n",
       "\n",
       "   mean_rec   mean_f1  \n",
       "0  0.685850  0.290855  \n",
       "1  0.661913  0.297880  \n",
       "2  0.684237  0.290517  \n",
       "3  0.684439  0.291051  \n",
       "4  0.661510  0.298040  \n",
       "5  0.684943  0.290832  \n",
       "6  0.686051  0.290543  \n",
       "7  0.664533  0.297723  \n",
       "8  0.683632  0.290245  \n",
       "9  0.684539  0.290977  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search1_results.sort_values(by='mean_roc_auc', ascending=False)[\n",
    "    ['params', 'mean_roc_auc', 'mean_prec', 'mean_rec', 'mean_f1']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we found decent lift (+0.02 ROC-AUC) from tuning the tree structure. We also virtually identical scores at the top, not just one outlier, which is a good sign we found an optimal structure -- if these top scores had similar parameters, that would be further proof:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_roc_auc</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>subsample</th>\n",
       "      <th>colsample_bytree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.781358</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.781235</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.781106</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.781089</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.781088</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_roc_auc  max_depth  min_child_weight  subsample  colsample_bytree\n",
       "0      0.781358        4.0               8.0       0.70               0.7\n",
       "1      0.781235        5.0               8.0       0.85               1.0\n",
       "2      0.781106        4.0              12.0       0.70               1.0\n",
       "3      0.781089        4.0              12.0       0.85               1.0\n",
       "4      0.781088        5.0               8.0       0.70               1.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_df = search1_results['params'].apply(pd.Series)\n",
    "results = pd.concat([search1_results.drop(columns='params'), params_df], axis=1)\n",
    "results.sort_values(by='mean_roc_auc', ascending=False)[\n",
    "    ['mean_roc_auc', 'max_depth', 'min_child_weight', 'subsample', 'colsample_bytree']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 5 models by ROC-AUC have very small changes in parameters -- The optimal region occurs with moderate `depth` (4–5), high `min_child_weight` (8–12), and subsampling between 0.7–0.85, suggesting strong generalization, reproducability, and minimal overfitting risk. We also don't see any sitting on a single edge, like if all the best performers were at the highest depth, so there is no reason to believe we are missing anything beyond our grid. \n",
    "\n",
    "These parameters will be fixed while tuning learning dynamics in the next phase:\n",
    "\n",
    "- max_depth = 4\n",
    "- min_child_weight = 8\n",
    "- subsample = 0.7\n",
    "- colsample_bytree = 1\n",
    "\n",
    "These were the parameters of the best model by ROC-AUC, *except* for `colsample_bytree` because four out of the top five models had this parameter set to 1, so I felt it was safest to keep it as that, even though the #1 model was 0.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Pass: Learning Dynamics\n",
    "\n",
    "Now that we have a reliable tree structure, we will focus on the parameters that effect how the model learns. \n",
    "\n",
    "I think in this case it makes more sense to do the wide-then-narrow search approach because the learning parameters are continuous and small changes can make a big difference. Therefore, we will first find the appropriate scale of each parameter, and then do a narrow search over a smaller range to find more exact optimal parameters. Also, the difference from the best to 2nd (or even 5th) best is so small it's negligble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding Scale of Learning Dynamics:**\n",
    "\n",
    "Bigger grid than before, so will take much longer. A reasonable option is to make 'reasonable' learning_rate + n_estimator pairings -- low learnings rates are very likely to underfit with a low number of estimators (trees) because it can't converge fast enough, so you can consider not running some of pairings you think are 'unreasonable'. \n",
    "\n",
    "However, I'm going to run this overnight either way and hopefully won't need to run again, so why not be thorough with the full grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/720 (  5.0% in 20.0 mins) | Best ROC-AUC: 0.7565 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.01, 'n_estimators': 300, 'gamma': 0.0, 'reg_alpha': 0.0, 'reg_lambda': 1.0}\n",
      "72/720 ( 10.0% in 46.7 mins) | Best ROC-AUC: 0.7660 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.01, 'n_estimators': 500, 'gamma': 0.0, 'reg_alpha': 0.5, 'reg_lambda': 1.0}\n",
      "108/720 ( 15.0% in 82.6 mins) | Best ROC-AUC: 0.7727 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.01, 'n_estimators': 800, 'gamma': 0.3, 'reg_alpha': 0.5, 'reg_lambda': 1.0}\n",
      "144/720 ( 20.0% in 134.2 mins) | Best ROC-AUC: 0.7772 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.01, 'n_estimators': 1200, 'gamma': 0.0, 'reg_alpha': 0.5, 'reg_lambda': 5.0}\n",
      "180/720 ( 25.0% in 149.6 mins) | Best ROC-AUC: 0.7772 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.01, 'n_estimators': 1200, 'gamma': 0.0, 'reg_alpha': 0.5, 'reg_lambda': 5.0}\n",
      "216/720 ( 30.0% in 172.2 mins) | Best ROC-AUC: 0.7787 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 500, 'gamma': 0.0, 'reg_alpha': 0.5, 'reg_lambda': 5.0}\n",
      "252/720 ( 35.0% in 205.5 mins) | Best ROC-AUC: 0.7818 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 800, 'gamma': 0.3, 'reg_alpha': 0.5, 'reg_lambda': 5.0}\n",
      "288/720 ( 40.0% in 252.7 mins) | Best ROC-AUC: 0.7831 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1200, 'gamma': 0.0, 'reg_alpha': 0.5, 'reg_lambda': 5.0}\n",
      "324/720 ( 45.0% in 267.5 mins) | Best ROC-AUC: 0.7831 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1200, 'gamma': 0.0, 'reg_alpha': 0.5, 'reg_lambda': 5.0}\n",
      "360/720 ( 50.0% in 289.5 mins) | Best ROC-AUC: 0.7831 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1200, 'gamma': 0.0, 'reg_alpha': 0.5, 'reg_lambda': 5.0}\n",
      "396/720 ( 55.0% in 321.5 mins) | Best ROC-AUC: 0.7831 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1200, 'gamma': 0.0, 'reg_alpha': 0.5, 'reg_lambda': 5.0}\n",
      "432/720 ( 60.0% in 367.6 mins) | Best ROC-AUC: 0.7831 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1200, 'gamma': 0.0, 'reg_alpha': 0.5, 'reg_lambda': 5.0}\n",
      "468/720 ( 65.0% in 382.0 mins) | Best ROC-AUC: 0.7831 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1200, 'gamma': 0.0, 'reg_alpha': 0.5, 'reg_lambda': 5.0}\n",
      "504/720 ( 70.0% in 403.7 mins) | Best ROC-AUC: 0.7831 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1200, 'gamma': 0.0, 'reg_alpha': 0.5, 'reg_lambda': 5.0}\n",
      "540/720 ( 75.0% in 435.5 mins) | Best ROC-AUC: 0.7831 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1200, 'gamma': 0.0, 'reg_alpha': 0.5, 'reg_lambda': 5.0}\n",
      "576/720 ( 80.0% in 481.2 mins) | Best ROC-AUC: 0.7831 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1200, 'gamma': 0.0, 'reg_alpha': 0.5, 'reg_lambda': 5.0}\n",
      "612/720 ( 85.0% in 495.3 mins) | Best ROC-AUC: 0.7831 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1200, 'gamma': 0.0, 'reg_alpha': 0.5, 'reg_lambda': 5.0}\n",
      "648/720 ( 90.0% in 516.6 mins) | Best ROC-AUC: 0.7831 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1200, 'gamma': 0.0, 'reg_alpha': 0.5, 'reg_lambda': 5.0}\n",
      "684/720 ( 95.0% in 548.3 mins) | Best ROC-AUC: 0.7831 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1200, 'gamma': 0.0, 'reg_alpha': 0.5, 'reg_lambda': 5.0}\n",
      "720/720 (100.0% in 599.0 mins) | Best ROC-AUC: 0.7831 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1200, 'gamma': 0.0, 'reg_alpha': 0.5, 'reg_lambda': 5.0}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "\n",
    "    # fix tree structure to optimal parameters found in pass 1\n",
    "    \"max_depth\": [4],\n",
    "    \"min_child_weight\": [8],\n",
    "    \"subsample\": [0.7],\n",
    "    \"colsample_bytree\": [1],\n",
    "\n",
    "    # wide grid to find scale of learning dynamics\n",
    "    \"learning_rate\": [0.01, 0.03, 0.05, 0.07, 0.10],\n",
    "    \"n_estimators\": [300, 500, 800, 1200],\n",
    "    \"gamma\": [0.0, 0.1, 0.2, 0.3],\n",
    "    \"reg_alpha\": [0.0, 0.5, 1.0],\n",
    "    \"reg_lambda\": [1.0, 2.0, 5.0]\n",
    "}\n",
    "\n",
    "search2_results = grid_search_xgb(apps_cv_strat, feature_cols, target_col, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save so we dont have to run it all again\n",
    "search2_results.to_csv('results/xgb_search2_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_roc_auc</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>gamma</th>\n",
       "      <th>reg_alpha</th>\n",
       "      <th>reg_lambda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.783057</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.782981</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.782968</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.782923</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.782863</td>\n",
       "      <td>0.05</td>\n",
       "      <td>800.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.782842</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.782842</td>\n",
       "      <td>0.05</td>\n",
       "      <td>800.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.782786</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.782783</td>\n",
       "      <td>0.05</td>\n",
       "      <td>800.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.782778</td>\n",
       "      <td>0.05</td>\n",
       "      <td>800.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_roc_auc  learning_rate  n_estimators  gamma  reg_alpha  reg_lambda\n",
       "0      0.783057           0.03        1200.0    0.0        0.5         5.0\n",
       "1      0.782981           0.03        1200.0    0.3        0.5         5.0\n",
       "2      0.782968           0.03        1200.0    0.1        0.5         5.0\n",
       "3      0.782923           0.03        1200.0    0.2        0.5         5.0\n",
       "4      0.782863           0.05         800.0    0.1        0.0         5.0\n",
       "5      0.782842           0.03        1200.0    0.2        0.5         1.0\n",
       "6      0.782842           0.05         800.0    0.0        0.0         5.0\n",
       "7      0.782786           0.03        1200.0    0.3        0.5         1.0\n",
       "8      0.782783           0.05         800.0    0.2        0.0         5.0\n",
       "9      0.782778           0.05         800.0    0.3        0.0         5.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_df = search2_results['params'].apply(pd.Series)\n",
    "results = pd.concat([search2_results.drop(columns='params'), params_df], axis=1)\n",
    "results.sort_values(by='mean_roc_auc', ascending=False)[\n",
    "    ['mean_roc_auc', 'learning_rate', 'n_estimators', 'gamma', 'reg_alpha', 'reg_lambda']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some lift again (+0.002) ROC-AUC, but much smaller than the last pass, which is expected, as we were just hoping to squeeze out points at this stage. Additionally, since all of the top scorers have almost identical performance, we can assume we are close to optimal learning dynamics.\n",
    "\n",
    "However, I notice that `n_estimators` and `reg_lambda` are consistently on an edge of our parameter range, so before doing our narrow search, I want to expand the scale of these a bit to see if they continue to operate better at higher scale. Also, `gamma` does not seem to have an optimal value at all, so I'll test a wider range for that as well, but suspecting it may not have a real impact on our trees. \n",
    "\n",
    "Note that increasing `n_estimators` has the biggest impact on speed, so that is something to consider if speed is required for this model -- but for now, we are just looking for best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/64 (  6.2% in 6.6 mins) | Best ROC-AUC: 0.7833 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1200, 'gamma': 0.0, 'reg_alpha': 0.5, 'reg_lambda': 15.0}\n",
      "7/64 ( 10.9% in 10.6 mins) | Best ROC-AUC: 0.7833 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1200, 'gamma': 0.0, 'reg_alpha': 0.5, 'reg_lambda': 15.0}\n",
      "10/64 ( 15.6% in 14.9 mins) | Best ROC-AUC: 0.7833 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1200, 'gamma': 0.2, 'reg_alpha': 0.5, 'reg_lambda': 15.0}\n",
      "13/64 ( 20.3% in 19.0 mins) | Best ROC-AUC: 0.7834 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1200, 'gamma': 0.4, 'reg_alpha': 0.5, 'reg_lambda': 15.0}\n",
      "16/64 ( 25.0% in 23.0 mins) | Best ROC-AUC: 0.7834 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1200, 'gamma': 0.4, 'reg_alpha': 0.5, 'reg_lambda': 15.0}\n",
      "20/64 ( 31.2% in 29.2 mins) | Best ROC-AUC: 0.7834 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1500, 'gamma': 0.0, 'reg_alpha': 0.5, 'reg_lambda': 15.0}\n",
      "23/64 ( 35.9% in 34.0 mins) | Best ROC-AUC: 0.7834 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1500, 'gamma': 0.0, 'reg_alpha': 0.5, 'reg_lambda': 15.0}\n",
      "26/64 ( 40.6% in 38.8 mins) | Best ROC-AUC: 0.7834 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1500, 'gamma': 0.0, 'reg_alpha': 0.5, 'reg_lambda': 15.0}\n",
      "29/64 ( 45.3% in 43.5 mins) | Best ROC-AUC: 0.7835 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1500, 'gamma': 0.4, 'reg_alpha': 0.5, 'reg_lambda': 15.0}\n",
      "32/64 ( 50.0% in 48.3 mins) | Best ROC-AUC: 0.7835 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1500, 'gamma': 0.6, 'reg_alpha': 0.5, 'reg_lambda': 15.0}\n",
      "36/64 ( 56.2% in 55.8 mins) | Best ROC-AUC: 0.7835 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1500, 'gamma': 0.6, 'reg_alpha': 0.5, 'reg_lambda': 15.0}\n",
      "39/64 ( 60.9% in 61.5 mins) | Best ROC-AUC: 0.7835 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1500, 'gamma': 0.6, 'reg_alpha': 0.5, 'reg_lambda': 15.0}\n",
      "42/64 ( 65.6% in 67.2 mins) | Best ROC-AUC: 0.7835 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1500, 'gamma': 0.6, 'reg_alpha': 0.5, 'reg_lambda': 15.0}\n",
      "45/64 ( 70.3% in 72.8 mins) | Best ROC-AUC: 0.7835 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1500, 'gamma': 0.6, 'reg_alpha': 0.5, 'reg_lambda': 15.0}\n",
      "48/64 ( 75.0% in 78.4 mins) | Best ROC-AUC: 0.7835 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1500, 'gamma': 0.6, 'reg_alpha': 0.5, 'reg_lambda': 15.0}\n",
      "52/64 ( 81.2% in 86.7 mins) | Best ROC-AUC: 0.7835 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1500, 'gamma': 0.6, 'reg_alpha': 0.5, 'reg_lambda': 15.0}\n",
      "55/64 ( 85.9% in 93.4 mins) | Best ROC-AUC: 0.7835 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1500, 'gamma': 0.6, 'reg_alpha': 0.5, 'reg_lambda': 15.0}\n",
      "58/64 ( 90.6% in 100.2 mins) | Best ROC-AUC: 0.7835 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1500, 'gamma': 0.6, 'reg_alpha': 0.5, 'reg_lambda': 15.0}\n",
      "61/64 ( 95.3% in 106.9 mins) | Best ROC-AUC: 0.7835 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1500, 'gamma': 0.6, 'reg_alpha': 0.5, 'reg_lambda': 15.0}\n",
      "64/64 (100.0% in 114.0 mins) | Best ROC-AUC: 0.7835 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1500, 'gamma': 0.6, 'reg_alpha': 0.5, 'reg_lambda': 15.0}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "\n",
    "    # fix tree structure to optimal parameters found in pass 1\n",
    "    \"max_depth\": [4],\n",
    "    \"min_child_weight\": [8],\n",
    "    \"subsample\": [0.7],\n",
    "    \"colsample_bytree\": [1],\n",
    "\n",
    "    \"learning_rate\": [0.03], # fixed, stable\n",
    "    \"n_estimators\": [1200, 1500, 1800, 2000], # expanding higher\n",
    "    \"gamma\": [0.0, 0.2, 0.4, 0.6], # trying some other values \n",
    "    \"reg_alpha\": [0.5], # fixed, stable\n",
    "    \"reg_lambda\": [5.0, 7.5, 10.0, 15.0] # expanding higher\n",
    "}\n",
    "\n",
    "search2_expanded_results = grid_search_xgb(apps_cv_strat, feature_cols, target_col, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save so we dont have to run it all again\n",
    "search2_expanded_results.to_csv('results/xgb_search2_expanded_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_roc_auc</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>gamma</th>\n",
       "      <th>reg_lambda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.783527</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.783494</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.783410</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.783397</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.783394</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.783341</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.783317</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.783302</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.783298</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.783287</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_roc_auc  n_estimators  gamma  reg_lambda\n",
       "0      0.783527        1500.0    0.6        15.0\n",
       "1      0.783494        1500.0    0.4        15.0\n",
       "2      0.783410        1500.0    0.0        15.0\n",
       "3      0.783397        1500.0    0.2        15.0\n",
       "4      0.783394        1200.0    0.4        15.0\n",
       "5      0.783341        1200.0    0.6        15.0\n",
       "6      0.783317        1800.0    0.4        15.0\n",
       "7      0.783302        1200.0    0.2        15.0\n",
       "8      0.783298        1800.0    0.6        15.0\n",
       "9      0.783287        1500.0    0.0        10.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_df = search2_expanded_results['params'].apply(pd.Series)\n",
    "results = pd.concat([search2_expanded_results.drop(columns='params'), params_df], axis=1)\n",
    "results.sort_values(by='mean_roc_auc', ascending=False)[\n",
    "    ['mean_roc_auc', 'n_estimators', 'gamma', 'reg_lambda']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `n_estimators` seems optimal at 1500\n",
    "- `gamma` appears to not affect the model. probably because we have so much regularization elsewhere that it doesn't matter much\n",
    "- `reg_lambda` is still at the top of our range so lets do one final sweep to see if any more performance can be squeezed out before diminishing returns\n",
    "\n",
    "A final search for `reg_lambda` scale before narrowing all parameters further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 ( 20.0% in 2.2 mins) | Best ROC-AUC: 0.7831 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1500, 'gamma': 1, 'reg_alpha': 0.5, 'reg_lambda': 10}\n",
      "2/5 ( 40.0% in 4.0 mins) | Best ROC-AUC: 0.7836 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1500, 'gamma': 1, 'reg_alpha': 0.5, 'reg_lambda': 15}\n",
      "3/5 ( 60.0% in 5.7 mins) | Best ROC-AUC: 0.7836 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1500, 'gamma': 1, 'reg_alpha': 0.5, 'reg_lambda': 15}\n",
      "4/5 ( 80.0% in 7.4 mins) | Best ROC-AUC: 0.7836 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1500, 'gamma': 1, 'reg_alpha': 0.5, 'reg_lambda': 15}\n",
      "5/5 (100.0% in 9.2 mins) | Best ROC-AUC: 0.7836 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1500, 'gamma': 1, 'reg_alpha': 0.5, 'reg_lambda': 15}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "\n",
    "    # fix tree structure to optimal parameters found in pass 1\n",
    "    \"max_depth\": [4],\n",
    "    \"min_child_weight\": [8],\n",
    "    \"subsample\": [0.7],\n",
    "    \"colsample_bytree\": [1],\n",
    "\n",
    "    \"learning_rate\": [0.03], # fixed, stable\n",
    "    \"n_estimators\": [1500], # fixed, stable\n",
    "    \"gamma\": [1], # fixed, not very useful\n",
    "    \"reg_alpha\": [0.5], # fixed, stable\n",
    "    \"reg_lambda\": [10, 15, 20, 25, 30] # further expansion\n",
    "}\n",
    "\n",
    "reg_lambda_expansion = grid_search_xgb(apps_cv_strat, feature_cols, target_col, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save so we dont have to run it all again\n",
    "reg_lambda_expansion.to_csv('results/xgb_reg_lambda_expansion_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_roc_auc</th>\n",
       "      <th>reg_lambda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.783609</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.783512</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.783464</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.783450</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.783123</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_roc_auc  reg_lambda\n",
       "0      0.783609        15.0\n",
       "1      0.783512        25.0\n",
       "2      0.783464        20.0\n",
       "3      0.783450        30.0\n",
       "4      0.783123        10.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_df = reg_lambda_expansion['params'].apply(pd.Series)\n",
    "results = pd.concat([reg_lambda_expansion.drop(columns='params'), params_df], axis=1)\n",
    "results.sort_values(by='mean_roc_auc', ascending=False)[\n",
    "    ['mean_roc_auc', 'reg_lambda']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we do peak at `reg_lambda`=15 before flattening out. \n",
    "\n",
    "I think we have found the appropriate scale for each of our learning dynamics hyperparameters:\n",
    "\n",
    "- `learning_rate` = 0.03\n",
    "- `n_estimators` = 1500\n",
    "- `gamma` = 1\n",
    "- `reg_alpha` = 0.5\n",
    "- `reg_lambda` = 15\n",
    "\n",
    "Now, a refined, narrow search around these parameters. Not expecting a huge lift here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding exact optimal parameters:**\n",
    "\n",
    "We have already determined that `gamma` does not affect much and that `reg_lambda` pleateaus hard at 15, so these do not need to be further fine-tuned. Changes due to `n_estimtors` tend to be pretty smooth, and we already tested with a step size of 300, so we can't expect too much of a difference, but a smaller step size could be worth trying. `learning_rate` and `reg_alpha` will be the primary parameters to refine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/45 (  6.7% in 5.5 mins) | Best ROC-AUC: 0.7828 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.02, 'n_estimators': 1350, 'gamma': 1, 'reg_alpha': 0.5, 'reg_lambda': 15}\n",
      "5/45 ( 11.1% in 9.2 mins) | Best ROC-AUC: 0.7828 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.02, 'n_estimators': 1350, 'gamma': 1, 'reg_alpha': 0.5, 'reg_lambda': 15}\n",
      "7/45 ( 15.6% in 12.7 mins) | Best ROC-AUC: 0.7832 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.02, 'n_estimators': 1500, 'gamma': 1, 'reg_alpha': 0.4, 'reg_lambda': 15}\n",
      "9/45 ( 20.0% in 16.3 mins) | Best ROC-AUC: 0.7832 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.02, 'n_estimators': 1500, 'gamma': 1, 'reg_alpha': 0.5, 'reg_lambda': 15}\n",
      "12/45 ( 26.7% in 21.6 mins) | Best ROC-AUC: 0.7835 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.02, 'n_estimators': 1650, 'gamma': 1, 'reg_alpha': 0.3, 'reg_lambda': 15}\n",
      "14/45 ( 31.1% in 25.0 mins) | Best ROC-AUC: 0.7835 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.02, 'n_estimators': 1650, 'gamma': 1, 'reg_alpha': 0.5, 'reg_lambda': 15}\n",
      "16/45 ( 35.6% in 28.2 mins) | Best ROC-AUC: 0.7835 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1350, 'gamma': 1, 'reg_alpha': 0.3, 'reg_lambda': 15}\n",
      "18/45 ( 40.0% in 31.1 mins) | Best ROC-AUC: 0.7836 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1350, 'gamma': 1, 'reg_alpha': 0.5, 'reg_lambda': 15}\n",
      "21/45 ( 46.7% in 35.5 mins) | Best ROC-AUC: 0.7836 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1350, 'gamma': 1, 'reg_alpha': 0.5, 'reg_lambda': 15}\n",
      "23/45 ( 51.1% in 38.7 mins) | Best ROC-AUC: 0.7836 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1350, 'gamma': 1, 'reg_alpha': 0.5, 'reg_lambda': 15}\n",
      "25/45 ( 55.6% in 41.9 mins) | Best ROC-AUC: 0.7836 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1350, 'gamma': 1, 'reg_alpha': 0.5, 'reg_lambda': 15}\n",
      "27/45 ( 60.0% in 45.4 mins) | Best ROC-AUC: 0.7836 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1350, 'gamma': 1, 'reg_alpha': 0.5, 'reg_lambda': 15}\n",
      "30/45 ( 66.7% in 50.5 mins) | Best ROC-AUC: 0.7836 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1350, 'gamma': 1, 'reg_alpha': 0.5, 'reg_lambda': 15}\n",
      "32/45 ( 71.1% in 53.3 mins) | Best ROC-AUC: 0.7836 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1350, 'gamma': 1, 'reg_alpha': 0.5, 'reg_lambda': 15}\n",
      "34/45 ( 75.6% in 56.1 mins) | Best ROC-AUC: 0.7836 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1350, 'gamma': 1, 'reg_alpha': 0.5, 'reg_lambda': 15}\n",
      "36/45 ( 80.0% in 59.1 mins) | Best ROC-AUC: 0.7836 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1350, 'gamma': 1, 'reg_alpha': 0.5, 'reg_lambda': 15}\n",
      "39/45 ( 86.7% in 63.8 mins) | Best ROC-AUC: 0.7836 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1350, 'gamma': 1, 'reg_alpha': 0.5, 'reg_lambda': 15}\n",
      "41/45 ( 91.1% in 67.2 mins) | Best ROC-AUC: 0.7836 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1350, 'gamma': 1, 'reg_alpha': 0.5, 'reg_lambda': 15}\n",
      "43/45 ( 95.6% in 71.2 mins) | Best ROC-AUC: 0.7836 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1350, 'gamma': 1, 'reg_alpha': 0.5, 'reg_lambda': 15}\n",
      "45/45 (100.0% in 75.3 mins) | Best ROC-AUC: 0.7836 | Best Params: {'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.7, 'colsample_bytree': 1, 'learning_rate': 0.03, 'n_estimators': 1350, 'gamma': 1, 'reg_alpha': 0.5, 'reg_lambda': 15}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "\n",
    "    # fix tree structure to optimal parameters found in pass 1\n",
    "    \"max_depth\": [4],\n",
    "    \"min_child_weight\": [8],\n",
    "    \"subsample\": [0.7],\n",
    "    \"colsample_bytree\": [1],\n",
    "\n",
    "    # very narrow search over the parameters we just found\n",
    "    \"learning_rate\": [0.02, 0.03, 0.04], \n",
    "    \"n_estimators\": [1350, 1500, 1650], \n",
    "    \"gamma\": [1], # fixed, not very useful\n",
    "    \"reg_alpha\": [0.3, 0.4, 0.5, 0.6, 0.7],\n",
    "    \"reg_lambda\": [15] \n",
    "}\n",
    "\n",
    "narrow_learning_search = grid_search_xgb(apps_cv_strat, feature_cols, target_col, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save so we dont have to run it all again\n",
    "narrow_learning_search.to_csv('results/xgb_narrow_learning_search_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_roc_auc</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>reg_alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.783621</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1350.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.783609</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.783561</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1650.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.783549</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1350.0</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.783547</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.783544</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.783534</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1350.0</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.783519</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1650.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.783482</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.783476</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1350.0</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_roc_auc  learning_rate  n_estimators  reg_alpha\n",
       "0      0.783621           0.03        1350.0        0.5\n",
       "1      0.783609           0.03        1500.0        0.5\n",
       "2      0.783561           0.03        1650.0        0.5\n",
       "3      0.783549           0.03        1350.0        0.3\n",
       "4      0.783547           0.03        1500.0        0.3\n",
       "5      0.783544           0.03        1500.0        0.7\n",
       "6      0.783534           0.03        1350.0        0.7\n",
       "7      0.783519           0.02        1650.0        0.5\n",
       "8      0.783482           0.03        1500.0        0.6\n",
       "9      0.783476           0.03        1350.0        0.6"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_df = narrow_learning_search['params'].apply(pd.Series)\n",
    "results = pd.concat([narrow_learning_search.drop(columns='params'), params_df], axis=1)\n",
    "results.sort_values(by='mean_roc_auc', ascending=False)[\n",
    "    ['mean_roc_auc', 'learning_rate', 'n_estimators', 'reg_alpha']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirmation\n",
    "\n",
    "Now I just want to confirm that these results are stable, not overfitting, and safe from leakage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stability across folds:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>acc</th>\n",
       "      <th>bal_acc</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>spec</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49156</td>\n",
       "      <td>1775</td>\n",
       "      <td>40409</td>\n",
       "      <td>4778</td>\n",
       "      <td>2194</td>\n",
       "      <td>0.858166</td>\n",
       "      <td>0.670739</td>\n",
       "      <td>0.270868</td>\n",
       "      <td>0.447216</td>\n",
       "      <td>0.894262</td>\n",
       "      <td>0.337388</td>\n",
       "      <td>0.784749</td>\n",
       "      <td>0.840518</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49156</td>\n",
       "      <td>1804</td>\n",
       "      <td>40240</td>\n",
       "      <td>4947</td>\n",
       "      <td>2165</td>\n",
       "      <td>0.855318</td>\n",
       "      <td>0.672522</td>\n",
       "      <td>0.267220</td>\n",
       "      <td>0.454523</td>\n",
       "      <td>0.890522</td>\n",
       "      <td>0.336567</td>\n",
       "      <td>0.783701</td>\n",
       "      <td>0.840971</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49156</td>\n",
       "      <td>1695</td>\n",
       "      <td>40200</td>\n",
       "      <td>4987</td>\n",
       "      <td>2274</td>\n",
       "      <td>0.852287</td>\n",
       "      <td>0.658348</td>\n",
       "      <td>0.253667</td>\n",
       "      <td>0.427060</td>\n",
       "      <td>0.889636</td>\n",
       "      <td>0.318280</td>\n",
       "      <td>0.770565</td>\n",
       "      <td>0.842926</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49155</td>\n",
       "      <td>1772</td>\n",
       "      <td>40178</td>\n",
       "      <td>5008</td>\n",
       "      <td>2197</td>\n",
       "      <td>0.853423</td>\n",
       "      <td>0.667815</td>\n",
       "      <td>0.261357</td>\n",
       "      <td>0.446460</td>\n",
       "      <td>0.889169</td>\n",
       "      <td>0.329705</td>\n",
       "      <td>0.782082</td>\n",
       "      <td>0.841722</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49154</td>\n",
       "      <td>1765</td>\n",
       "      <td>40352</td>\n",
       "      <td>4834</td>\n",
       "      <td>2203</td>\n",
       "      <td>0.856838</td>\n",
       "      <td>0.668914</td>\n",
       "      <td>0.267465</td>\n",
       "      <td>0.444808</td>\n",
       "      <td>0.893020</td>\n",
       "      <td>0.334059</td>\n",
       "      <td>0.777351</td>\n",
       "      <td>0.841928</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       n    tp     tn    fp    fn       acc   bal_acc      prec       rec  \\\n",
       "0  49156  1775  40409  4778  2194  0.858166  0.670739  0.270868  0.447216   \n",
       "1  49156  1804  40240  4947  2165  0.855318  0.672522  0.267220  0.454523   \n",
       "2  49156  1695  40200  4987  2274  0.852287  0.658348  0.253667  0.427060   \n",
       "3  49155  1772  40178  5008  2197  0.853423  0.667815  0.261357  0.446460   \n",
       "4  49154  1765  40352  4834  2203  0.856838  0.668914  0.267465  0.444808   \n",
       "\n",
       "       spec        f1   roc_auc  train_roc_auc  fold  \n",
       "0  0.894262  0.337388  0.784749       0.840518     1  \n",
       "1  0.890522  0.336567  0.783701       0.840971     2  \n",
       "2  0.889636  0.318280  0.770565       0.842926     3  \n",
       "3  0.889169  0.329705  0.782082       0.841722     4  \n",
       "4  0.893020  0.334059  0.777351       0.841928     5  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_params = {\n",
    "\n",
    "    # tree structure\n",
    "    \"max_depth\": 4,\n",
    "    \"min_child_weight\": 8,\n",
    "    \"subsample\": 0.7,\n",
    "    \"colsample_bytree\": 1,\n",
    "\n",
    "    # learning dynamics\n",
    "    \"learning_rate\": 0.03, \n",
    "    \"n_estimators\": 1350, \n",
    "    \"gamma\": 1,\n",
    "    \"reg_alpha\": 0.5,\n",
    "    \"reg_lambda\": 15 \n",
    "\n",
    "    # rest are already set in cv function\n",
    "}\n",
    "\n",
    "fold_metrics, preds = cv_xgb(apps_cv_strat, feature_cols, target_col, tuned_params)\n",
    "fold_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUC: 0.779690\n",
      "Std AUC: 0.005834\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean AUC: {fold_metrics.roc_auc.mean():.6f}\")\n",
    "print(f\"Std AUC: {fold_metrics.roc_auc.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appears to be extremely stable. Very small differences across folds, meaning that the average score isn't carried by just one fold getting 'lucky'. We can assume consistent performance from our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overfit check:**\n",
    "\n",
    "Want to see a minimal gap between trainings and testing ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train AUC: 0.8416130608394795\n",
      "Mean Val AUC: 0.7796898608913626\n",
      "Gap: 0.0619231999481169\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Train AUC:\", fold_metrics.train_roc_auc.mean())\n",
    "print(\"Mean Val AUC:\", fold_metrics.roc_auc.mean())\n",
    "print(\"Gap:\", fold_metrics.train_roc_auc.mean() - fold_metrics.roc_auc.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is definitely a little bit of overfit, but the stability across folds also mitigates this risk. We can try adding some regularization / making the tree less complex to minimize that gap while keeping performance high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train AUC: 0.8245869891682421\n",
      "Mean Val AUC: 0.7789686872814786\n",
      "Gap: 0.045618301886763524\n",
      "Std AUC: 0.006055\n"
     ]
    }
   ],
   "source": [
    "final_params = {\n",
    "\n",
    "    # tree structure\n",
    "    \"max_depth\": 4,\n",
    "    \"min_child_weight\": 8,\n",
    "    \"subsample\": 0.5,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "\n",
    "    # learning dynamics\n",
    "    \"learning_rate\": 0.03, \n",
    "    \"n_estimators\": 1000, \n",
    "    \"gamma\": 2,\n",
    "    \"reg_alpha\": 0.5,\n",
    "    \"reg_lambda\": 20 \n",
    "\n",
    "    # rest are already set in cv function\n",
    "}\n",
    "fold_metrics, preds = cv_xgb(apps_cv_strat, feature_cols, target_col, final_params)\n",
    "print(\"Mean Train AUC:\", fold_metrics.train_roc_auc.mean())\n",
    "print(\"Mean Val AUC:\", fold_metrics.roc_auc.mean())\n",
    "print(\"Gap:\", fold_metrics.train_roc_auc.mean() - fold_metrics.roc_auc.mean())\n",
    "print(f\"Std AUC: {fold_metrics.roc_auc.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making a few tweaks to the complexity of the model, we have closed the gap between training and validation performance by 26% without affecting validation performance in a significant way. I believe the gap is now within a comfortable range to assume we are *not* overfitting. Variance between folds even decreased a bit too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leakage Check:**\n",
    "\n",
    "Lastly, I want to confirm that we have a *safe* model by confirming as best as possible that we evaluated our model properly, especially by not revealing information from our test set to our training set.\n",
    "\n",
    "One way to assess this is by shuffling the target variable, removing any structural patterns from the data, which means we should expect ~0.5 AUC (randomly guessing). If we see significantly higher, it would indicate that the model is exploiting some sort of pattern that it could only pick up on through data leaking from testing to training.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled mean AUC: 0.49585671451650304\n"
     ]
    }
   ],
   "source": [
    "shuffled = apps_cv_strat.copy()\n",
    "shuffled['TARGET'] = np.random.permutation(shuffled['TARGET'].values)\n",
    "fold_metrics_shuffled, preds_shuffled = cv_xgb(shuffled, feature_cols, target_col, final_params)\n",
    "print(\"Shuffled mean AUC:\", fold_metrics_shuffled.roc_auc.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, no issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Hyperparameters\n",
    "\n",
    "Now that we have confirmed a safe and reliabel model, reducing risk of overfitting and leakage, I think we are comfortable with these hyperparameters (including the pre-set params):\n",
    "\n",
    "- `max_depth` = 4\n",
    "- `min_child_weight` = 8\n",
    "- `subsample` = 0.5\n",
    "- `colsample_bytree` = 0.7\n",
    "- `learning_rate` = 0.03\n",
    "- `n_estimators` = 1000\n",
    "- `gamma` = 2\n",
    "- `reg_alpha` = 0.5\n",
    "- `reg_lambda` = 20\n",
    "- `scale_pos_weight` = 0.5 * #neg/#pos (found the 0.5 scaler helped a little)\n",
    "- `eval_metric` = 'auc'\n",
    "- `tree_method` = 'hist'\n",
    "- `random_state` = 42\n",
    "- `n_jobs` = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Holdout Evaluation\n",
    "\n",
    "We are going to compare this model to other models through performance on the holdout set. To do this, we will train on the set model above on the entire dataset we have been using so far for model selection, and then test it on the entire holdout set that we have not seen yet for unbiased evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n': 61443,\n",
       " 'tp': 2200,\n",
       " 'tn': 50282,\n",
       " 'fp': 6201,\n",
       " 'fn': 2760,\n",
       " 'acc': 0.8541575118402421,\n",
       " 'bal_acc': 0.666881570989387,\n",
       " 'prec': 0.26187358647780024,\n",
       " 'rec': 0.4435483870967742,\n",
       " 'spec': 0.8902147548819999,\n",
       " 'f1': 0.32931666791407826,\n",
       " 'roc_auc': 0.7859435296832104,\n",
       " 'train_roc_auc': 0.8174978218587409}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate data\n",
    "X_train, y_train = apps_cv_strat[feature_cols], apps_cv_strat[target_col]\n",
    "X_test, y_test = apps_holdout_strat[feature_cols], apps_holdout_strat[target_col]\n",
    "\n",
    "# initiliaze our finalized parameters\n",
    "pos = (y_train == 1).sum()\n",
    "neg = (y_train == 0).sum()\n",
    "params = {\n",
    "    \"max_depth\": 4,\n",
    "    \"min_child_weight\": 8,\n",
    "    \"subsample\": 0.5,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \"learning_rate\": 0.03, \n",
    "    \"n_estimators\": 1000, \n",
    "    \"gamma\": 2,\n",
    "    \"reg_alpha\": 0.5,\n",
    "    \"reg_lambda\": 20,\n",
    "    'eval_metric': 'auc', \n",
    "    'random_state': 42, \n",
    "    'n_jobs': -1, \n",
    "    'tree_method': 'hist',\n",
    "    'scale_pos_weight': (neg / max(pos, 1)) * 0.5 \n",
    "}\n",
    "\n",
    "# fit model on training data\n",
    "model = XGBClassifier(**params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict on holdout data\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "y_pred  = (y_prob >= 0.5).astype(int) # just the default 0.5 threshold for now\n",
    "y_train_prob = model.predict_proba(X_train)[:,1] # make training predictions as well to assess overfit\n",
    "\n",
    "# calculate classification metrics from previously defined functions\n",
    "metrics = classification_metrics(y_test, y_pred)\n",
    "metrics['roc_auc'] = roc_auc_from_probs(y_test, y_prob)\n",
    "metrics['train_roc_auc'] = roc_auc_from_probs(y_train, y_train_prob)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we still haven't tuned threshold, we are really only looking at ROC-AUC right now. \n",
    "\n",
    "The performance from cross-validation carried over to the holdout set (with small improvement even), indicating we have a very generalizable model with no leakage risks. Overfitting is still at low-risk. Overall, a successful run on the holdout set, and I'm pretty confident we squeezed out as much performance as we can from this XGBoost model.\n",
    "\n",
    "An `ROC-AUC` of **0.784** is what we will compare to our other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Predictions\n",
    "\n",
    "The Kaggle competition has it's own holdout set that we need to predict on for our submission. This doesn't have the true targets so we can't evaluate it ourselves, but we can make the predictions and submit it for an overall score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the full training and test data\n",
    "## AFTER running through our pipeline files\n",
    "train = pd.read_csv('data/apps_all_background.csv')\n",
    "test = pd.read_csv('data/apps_all_background_test.csv')\n",
    "\n",
    "# separate data\n",
    "X_train, y_train = train[feature_cols], train[target_col]\n",
    "X_test = test[feature_cols]\n",
    "\n",
    "# fit model on training data\n",
    "model = XGBClassifier(**params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict on holdout data\n",
    "test[\"TARGET\"] = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# create submission csv\n",
    "submission = test[[\"SK_ID_CURR\", \"TARGET\"]]\n",
    "submission.to_csv(\"results/submission_xgb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS:** \n",
    "- `Public`: 0.788\n",
    "- `Private`: 0.781"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another submission with the parameters *before* adjusting for overfit just to try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_depth\": 4,\n",
    "    \"min_child_weight\": 8,\n",
    "    \"subsample\": 0.7,\n",
    "    \"colsample_bytree\": 1,\n",
    "    \"learning_rate\": 0.03, \n",
    "    \"n_estimators\": 1350, \n",
    "    \"gamma\": 1,\n",
    "    \"reg_alpha\": 0.5,\n",
    "    \"reg_lambda\": 15,\n",
    "    'eval_metric': 'auc', \n",
    "    'random_state': 42, \n",
    "    'n_jobs': -1, \n",
    "    'tree_method': 'hist',\n",
    "    'scale_pos_weight': (neg / max(pos, 1)) * 0.5 \n",
    "}\n",
    "\n",
    "# fit model on training data\n",
    "model = XGBClassifier(**params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict on holdout data\n",
    "test[\"TARGET\"] = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# create submission csv\n",
    "submission = test[[\"SK_ID_CURR\", \"TARGET\"]]\n",
    "submission.to_csv(\"results/submission2_xgb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS:** \n",
    "- `Public`: 0.790\n",
    "- `Private`: 0.783"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to test the xgboost model on just the given application data on our holdout set as an experiment so we can measure lift from feature engineering + lift from tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_apps_train shape: (245777, 119)\n"
     ]
    }
   ],
   "source": [
    "# read in original applications table\n",
    "apps = pd.read_csv(\"data/application_train.csv\")  \n",
    "\n",
    "# get id's from current tables we're using to matchup with original tables\n",
    "train_prev = pd.read_csv(\"data/apps_cv_strat.csv\", usecols=[\"SK_ID_CURR\"])\n",
    "hold_prev  = pd.read_csv(\"data/apps_holdout_strat.csv\",   usecols=[\"SK_ID_CURR\"])\n",
    "train_ids = set(train_prev[\"SK_ID_CURR\"])\n",
    "hold_ids  = set(hold_prev[\"SK_ID_CURR\"])\n",
    "\n",
    "# split original applications data based on how we already split current data\n",
    "apps_train = apps[apps[\"SK_ID_CURR\"].isin(train_ids)].copy()\n",
    "apps_hold  = apps[apps[\"SK_ID_CURR\"].isin(hold_ids)].copy()\n",
    "\n",
    "# add in folds for cross validation\n",
    "folds = pd.read_csv(\"data/apps_cv_strat.csv\", usecols=[\"SK_ID_CURR\", \"fold\"])\n",
    "apps_train = apps_train.merge(folds, on=\"SK_ID_CURR\", how=\"left\")\n",
    "\n",
    "# cast objects to category (xgboost can handle it)\n",
    "for df in (apps_train, apps_hold):\n",
    "    for c in df.select_dtypes(include=\"object\").columns:\n",
    "        df[c] = df[c].astype(\"category\")\n",
    "\n",
    "target_col = \"TARGET\"\n",
    "feature_cols_apps = [c for c in apps.columns if c not in [\"SK_ID_CURR\", target_col, \"fold\", 'CODE_GENDER_M', 'CODE_GENDER', 'AGE_INT']]\n",
    "X_apps_train, y_apps_train = apps_train[feature_cols_apps], apps_train[target_col]\n",
    "\n",
    "print(\"X_apps_train shape:\", X_apps_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>acc</th>\n",
       "      <th>bal_acc</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>spec</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>train_roc_auc</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49156</td>\n",
       "      <td>1421</td>\n",
       "      <td>40704</td>\n",
       "      <td>4483</td>\n",
       "      <td>2548</td>\n",
       "      <td>0.856966</td>\n",
       "      <td>0.629407</td>\n",
       "      <td>0.240684</td>\n",
       "      <td>0.358025</td>\n",
       "      <td>0.900790</td>\n",
       "      <td>0.287856</td>\n",
       "      <td>0.738588</td>\n",
       "      <td>0.897444</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49156</td>\n",
       "      <td>1389</td>\n",
       "      <td>40722</td>\n",
       "      <td>4465</td>\n",
       "      <td>2580</td>\n",
       "      <td>0.856681</td>\n",
       "      <td>0.625575</td>\n",
       "      <td>0.237274</td>\n",
       "      <td>0.349962</td>\n",
       "      <td>0.901188</td>\n",
       "      <td>0.282806</td>\n",
       "      <td>0.736199</td>\n",
       "      <td>0.895527</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49156</td>\n",
       "      <td>1359</td>\n",
       "      <td>40614</td>\n",
       "      <td>4573</td>\n",
       "      <td>2610</td>\n",
       "      <td>0.853873</td>\n",
       "      <td>0.620601</td>\n",
       "      <td>0.229096</td>\n",
       "      <td>0.342404</td>\n",
       "      <td>0.898798</td>\n",
       "      <td>0.274518</td>\n",
       "      <td>0.724889</td>\n",
       "      <td>0.894448</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49155</td>\n",
       "      <td>1431</td>\n",
       "      <td>40571</td>\n",
       "      <td>4615</td>\n",
       "      <td>2538</td>\n",
       "      <td>0.854481</td>\n",
       "      <td>0.629205</td>\n",
       "      <td>0.236685</td>\n",
       "      <td>0.360544</td>\n",
       "      <td>0.897867</td>\n",
       "      <td>0.285771</td>\n",
       "      <td>0.736082</td>\n",
       "      <td>0.896550</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49154</td>\n",
       "      <td>1433</td>\n",
       "      <td>40564</td>\n",
       "      <td>4622</td>\n",
       "      <td>2535</td>\n",
       "      <td>0.854396</td>\n",
       "      <td>0.629425</td>\n",
       "      <td>0.236664</td>\n",
       "      <td>0.361139</td>\n",
       "      <td>0.897712</td>\n",
       "      <td>0.285942</td>\n",
       "      <td>0.728191</td>\n",
       "      <td>0.894462</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       n    tp     tn    fp    fn       acc   bal_acc      prec       rec  \\\n",
       "0  49156  1421  40704  4483  2548  0.856966  0.629407  0.240684  0.358025   \n",
       "1  49156  1389  40722  4465  2580  0.856681  0.625575  0.237274  0.349962   \n",
       "2  49156  1359  40614  4573  2610  0.853873  0.620601  0.229096  0.342404   \n",
       "3  49155  1431  40571  4615  2538  0.854481  0.629205  0.236685  0.360544   \n",
       "4  49154  1433  40564  4622  2535  0.854396  0.629425  0.236664  0.361139   \n",
       "\n",
       "       spec        f1   roc_auc  train_roc_auc  fold  \n",
       "0  0.900790  0.287856  0.738588       0.897444     1  \n",
       "1  0.901188  0.282806  0.736199       0.895527     2  \n",
       "2  0.898798  0.274518  0.724889       0.894448     3  \n",
       "3  0.897867  0.285771  0.736082       0.896550     4  \n",
       "4  0.897712  0.285942  0.728191       0.894462     5  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_data_metrics, og_data_preds = cv_xgb(apps_train, feature_cols_apps, target_col, params={'enable_categorical':True})\n",
    "og_data_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So overall for our 3 main stages of improvement:\n",
    "\n",
    "| Stage                            | Description                                                                          | Key Changes                                                                   |    Mean ROC-AUC   |              Approx. Lift from Base             |\n",
    "| :------------------------------- | :----------------------------------------------------------------------------------- | :---------------------------------------------------------------------------- | :---------------: | :-----------------------------------: |\n",
    "| **1. Base** | Model trained only on the raw `application_train.csv` features.                      | No joins or aggregations. Baseline feature set.                               | **~0.73** |                                      |\n",
    "| **2. Feature-Engineered**       | Added engineered + aggregated variables from internal / external credit tables.      | Joins, aggregations, missing-value flags, structural imputations.                           | **~0.76** |               **+0.026**              |\n",
    "| **3️. Tuned XGBoost**            | Final optimized parameters (depth = 4, min_child_weight = 8, subsample = 0.5, etc.). | Full cross-validated tuning on tree structure + learning dynamics. | **~0.786** | **+0.047** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Threshold Tuning\n",
    "\n",
    "With a solid ROC-AUC, we know that our model can rank applicants based on risk well, but the cutoff for decision-making is highly business-dependent. The main question that needs to be considered is the cost of mislabeling in both directions. We can attempt to answer this on our own and come up with a proposed solution, but it can also be something we leave a bit open-ended in our report asking for feedback on what the client would prefer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try 200 thresholds evenly spaced from 0 to 1\n",
    "## should be fast. no model required with the predictions we already have\n",
    "\n",
    "thresholds = np.linspace(0, 1, 200)\n",
    "scores = []\n",
    "for t in thresholds:\n",
    "    y_pred = (preds[\"y_prob\"] >= t).astype(int)\n",
    "    tp = ((y_pred == 1) & (preds[\"y_true\"] == 1)).sum()\n",
    "    fp = ((y_pred == 1) & (preds[\"y_true\"] == 0)).sum()\n",
    "    tn = ((y_pred == 0) & (preds[\"y_true\"] == 0)).sum()\n",
    "    fn = ((y_pred == 0) & (preds[\"y_true\"] == 1)).sum()\n",
    "\n",
    "    prec = tp / max(tp + fp, 1)\n",
    "    rec  = tp / max(tp + fn, 1)\n",
    "    f1   = 2 * prec * rec / max(prec + rec, 1e-12)\n",
    "    spec = tn / max(tn + fp, 1)\n",
    "    bal_acc = (rec + spec) / 2\n",
    "\n",
    "    scores.append((t, prec, rec, f1, bal_acc))\n",
    "\n",
    "scores_df = pd.DataFrame(scores, columns=[\"threshold\", \"precision\", \"recall\", \"f1\", \"bal_acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorted by precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>bal_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.934673</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.000554</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>0.500275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.939698</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.500149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.924623</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.002415</td>\n",
       "      <td>0.500596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.929648</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>0.500371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0.919598</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.003519</td>\n",
       "      <td>0.500858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     threshold  precision    recall        f1   bal_acc\n",
       "186   0.934673   0.916667  0.000554  0.001108  0.500275\n",
       "187   0.939698   0.857143  0.000302  0.000605  0.500149\n",
       "184   0.924623   0.857143  0.001209  0.002415  0.500596\n",
       "185   0.929648   0.833333  0.000756  0.001510  0.500371\n",
       "183   0.919598   0.760870  0.001764  0.003519  0.500858"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df[scores_df.precision != 1].sort_values(\"precision\", ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorted by recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>bal_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.020101</td>\n",
       "      <td>0.080914</td>\n",
       "      <td>0.999899</td>\n",
       "      <td>0.149713</td>\n",
       "      <td>0.501171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.025126</td>\n",
       "      <td>0.081159</td>\n",
       "      <td>0.999698</td>\n",
       "      <td>0.150129</td>\n",
       "      <td>0.502805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.030151</td>\n",
       "      <td>0.081536</td>\n",
       "      <td>0.999395</td>\n",
       "      <td>0.150771</td>\n",
       "      <td>0.505308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.035176</td>\n",
       "      <td>0.082028</td>\n",
       "      <td>0.998841</td>\n",
       "      <td>0.151606</td>\n",
       "      <td>0.508534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.040201</td>\n",
       "      <td>0.082666</td>\n",
       "      <td>0.998085</td>\n",
       "      <td>0.152686</td>\n",
       "      <td>0.512651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  precision    recall        f1   bal_acc\n",
       "4   0.020101   0.080914  0.999899  0.149713  0.501171\n",
       "5   0.025126   0.081159  0.999698  0.150129  0.502805\n",
       "6   0.030151   0.081536  0.999395  0.150771  0.505308\n",
       "7   0.035176   0.082028  0.998841  0.151606  0.508534\n",
       "8   0.040201   0.082666  0.998085  0.152686  0.512651"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df[scores_df.recall != 1].sort_values(\"recall\", ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorted by F1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>bal_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.512563</td>\n",
       "      <td>0.268768</td>\n",
       "      <td>0.428492</td>\n",
       "      <td>0.330335</td>\n",
       "      <td>0.663050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.507538</td>\n",
       "      <td>0.265817</td>\n",
       "      <td>0.435295</td>\n",
       "      <td>0.330073</td>\n",
       "      <td>0.664849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.527638</td>\n",
       "      <td>0.277585</td>\n",
       "      <td>0.406823</td>\n",
       "      <td>0.330002</td>\n",
       "      <td>0.656916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.502513</td>\n",
       "      <td>0.263148</td>\n",
       "      <td>0.442250</td>\n",
       "      <td>0.329962</td>\n",
       "      <td>0.666741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.517588</td>\n",
       "      <td>0.271264</td>\n",
       "      <td>0.420429</td>\n",
       "      <td>0.329763</td>\n",
       "      <td>0.660614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     threshold  precision    recall        f1   bal_acc\n",
       "102   0.512563   0.268768  0.428492  0.330335  0.663050\n",
       "101   0.507538   0.265817  0.435295  0.330073  0.664849\n",
       "105   0.527638   0.277585  0.406823  0.330002  0.656916\n",
       "100   0.502513   0.263148  0.442250  0.329962  0.666741\n",
       "103   0.517588   0.271264  0.420429  0.329763  0.660614"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df.sort_values(\"f1\", ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorted by balanced accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>bal_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.321608</td>\n",
       "      <td>0.177605</td>\n",
       "      <td>0.705201</td>\n",
       "      <td>0.283747</td>\n",
       "      <td>0.709197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.316583</td>\n",
       "      <td>0.175505</td>\n",
       "      <td>0.712004</td>\n",
       "      <td>0.281597</td>\n",
       "      <td>0.709109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.326633</td>\n",
       "      <td>0.179645</td>\n",
       "      <td>0.698246</td>\n",
       "      <td>0.285767</td>\n",
       "      <td>0.709095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.311558</td>\n",
       "      <td>0.173513</td>\n",
       "      <td>0.718656</td>\n",
       "      <td>0.279534</td>\n",
       "      <td>0.708998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.306533</td>\n",
       "      <td>0.171554</td>\n",
       "      <td>0.725509</td>\n",
       "      <td>0.277492</td>\n",
       "      <td>0.708895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    threshold  precision    recall        f1   bal_acc\n",
       "64   0.321608   0.177605  0.705201  0.283747  0.709197\n",
       "63   0.316583   0.175505  0.712004  0.281597  0.709109\n",
       "65   0.326633   0.179645  0.698246  0.285767  0.709095\n",
       "62   0.311558   0.173513  0.718656  0.279534  0.708998\n",
       "61   0.306533   0.171554  0.725509  0.277492  0.708895"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df.sort_values(\"bal_acc\", ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously there is a big tradeoff between precision and recall. For the rest of our evaluation, we will continue with continue with balanced accuracy as our optimizing metric, but will give other options as:\n",
    "\n",
    "| Goal                                  | Choose threshold | Reason                                                                      |\n",
    "| ------------------------------------- | ---------------- | ------------------------------------------------------------------------------ |\n",
    "| **Balanced performance**              | ~ 0.30      | Maximizes Balanced Accuracy (~0.71) and keeps F1 moderate.                       |\n",
    "| **Cautious - low false alarms**       | ≥ 0.45           | Higher precision, lower recall - good if rejecting loans is costly.            |\n",
    "| **Aggressive - flag all defaulters** | ≤ 0.20           | Boosts recall but also false positives - good for an initial screen followed by human review. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And anywhere inbetween these ranges to balance different objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Fairness Evaluation\n",
    "\n",
    "The groups from our data we are interested in are gender (male vs. female) and age (binned into decades). Note that these features have been removed from our model for *Group Unawarness*. Since unknown proxy attributes may exist, we will compute various metrics as confirmation.\n",
    "\n",
    "After fixing the decision threshold at 0.3 (chosen to maximize balanced accuracy), we will evaluate whether the model treats these groups equitably using several fairness metrics from class:\n",
    "\n",
    "- Demographic Parity: compares overall positive prediction rates across groups\n",
    "- Equal Opportunity: compares true positive rates (recall) across groups\n",
    "- Equalized Odds: compares both true and false positive rates across groups\n",
    "- Predictive Parity (PPVP): compares precision across groups\n",
    "\n",
    "These metrics will be computed on the holdout predictions to assess if any group is disproportionately disadvantaged by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, fit holdout data again and get demographic groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate data\n",
    "X_train, y_train = apps_cv_strat[feature_cols], apps_cv_strat[target_col]\n",
    "X_test, y_test = apps_holdout_strat[feature_cols], apps_holdout_strat[target_col]\n",
    "\n",
    "# initiliaze our finalized parameters\n",
    "pos = (y_train == 1).sum()\n",
    "neg = (y_train == 0).sum()\n",
    "params = {\n",
    "    \"max_depth\": 4,\n",
    "    \"min_child_weight\": 8,\n",
    "    \"subsample\": 0.5,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \"learning_rate\": 0.03, \n",
    "    \"n_estimators\": 1000, \n",
    "    \"gamma\": 2,\n",
    "    \"reg_alpha\": 0.5,\n",
    "    \"reg_lambda\": 20,\n",
    "    'eval_metric': 'auc', \n",
    "    'random_state': 42, \n",
    "    'n_jobs': -1, \n",
    "    'tree_method': 'hist',\n",
    "    'scale_pos_weight': (neg / max(pos, 1)) * 0.5 \n",
    "}\n",
    "\n",
    "# fit model on training data\n",
    "model = XGBClassifier(**params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict on holdout data\n",
    "apps_holdout_strat['y_prob'] = model.predict_proba(X_test)[:, 1]\n",
    "apps_holdout_strat['y_pred']  = (apps_holdout_strat['y_prob'] >= 0.3).astype(int) # optimal 0.3 threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert gender back to readable label\n",
    "apps_holdout_strat['GENDER'] = np.where(apps_holdout_strat['CODE_GENDER_M'], 'Male', 'Female')\n",
    "\n",
    "# convert marital status to readable label\n",
    "status_cols = [c for c in apps_holdout_strat.columns if c.startswith(\"NAME_FAMILY_STATUS_\")]\n",
    "def get_status(row):\n",
    "    for c in status_cols:\n",
    "        if row[c] == 1:\n",
    "            return c.replace(\"NAME_FAMILY_STATUS_\", \"\")\n",
    "    return \"Married\"\n",
    "apps_holdout_strat[\"MARITAL_STATUS\"] = apps_holdout_strat.apply(get_status, axis=1)\n",
    "\n",
    "\n",
    "# bin ages by decades\n",
    "bins   = [20,30,40,50,60,70]\n",
    "labels = [\"20–29\",\"30–39\",\"40–49\",\"50–59\",\"60–69\"]\n",
    "apps_holdout_strat['AGE_GROUP'] = pd.cut(apps_holdout_strat['AGE_INT'], bins=bins, labels=labels, right=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function for fairness metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to caluclate fairness metrics based on a protected group\n",
    "def fairness_metrics(df, group_col, y_true=\"TARGET\", y_pred=\"y_pred\"):\n",
    "    results = []\n",
    "    for g, sub in df.groupby(group_col):\n",
    "        tp = ((sub[y_pred]==1) & (sub[y_true]==1)).sum()\n",
    "        fp = ((sub[y_pred]==1) & (sub[y_true]==0)).sum()\n",
    "        tn = ((sub[y_pred]==0) & (sub[y_true]==0)).sum()\n",
    "        fn = ((sub[y_pred]==0) & (sub[y_true]==1)).sum()\n",
    "        \n",
    "        # core rates\n",
    "        pos_rate = (tp + fp) / max(len(sub), 1)\n",
    "        tpr = tp / max(tp + fn, 1)   # recall\n",
    "        fpr = fp / max(fp + tn, 1)\n",
    "        prec = tp / max(tp + fp, 1)\n",
    "        \n",
    "        results.append({\n",
    "            group_col: g,\n",
    "            \"n\": len(sub),\n",
    "            \"Pos Rate\": pos_rate,\n",
    "            \"TPR (Equal Opportunity)\": tpr,\n",
    "            \"FPR\": fpr,\n",
    "            \"Precision (PPVP)\": prec,\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df['TPR_Ratio'] = results_df['TPR (Equal Opportunity)'] / results_df['TPR (Equal Opportunity)'].max()\n",
    "    results_df['FPR_Ratio'] = results_df['FPR'] / results_df['FPR'].max()\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GENDER</th>\n",
       "      <th>n</th>\n",
       "      <th>Pos Rate</th>\n",
       "      <th>TPR (Equal Opportunity)</th>\n",
       "      <th>FPR</th>\n",
       "      <th>Precision (PPVP)</th>\n",
       "      <th>TPR_Ratio</th>\n",
       "      <th>FPR_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>40608</td>\n",
       "      <td>0.327842</td>\n",
       "      <td>0.741143</td>\n",
       "      <td>0.296634</td>\n",
       "      <td>0.158717</td>\n",
       "      <td>0.962482</td>\n",
       "      <td>0.807144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>20835</td>\n",
       "      <td>0.408255</td>\n",
       "      <td>0.770033</td>\n",
       "      <td>0.367510</td>\n",
       "      <td>0.190924</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GENDER      n  Pos Rate  TPR (Equal Opportunity)       FPR  \\\n",
       "0  Female  40608  0.327842                 0.741143  0.296634   \n",
       "1    Male  20835  0.408255                 0.770033  0.367510   \n",
       "\n",
       "   Precision (PPVP)  TPR_Ratio  FPR_Ratio  \n",
       "0          0.158717   0.962482   0.807144  \n",
       "1          0.190924   1.000000   1.000000  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fairness_metrics(apps_holdout_strat, 'GENDER', 'TARGET', 'y_pred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All metrics are higher for men, but all are relatively close. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE_GROUP</th>\n",
       "      <th>n</th>\n",
       "      <th>Pos Rate</th>\n",
       "      <th>TPR (Equal Opportunity)</th>\n",
       "      <th>FPR</th>\n",
       "      <th>Precision (PPVP)</th>\n",
       "      <th>TPR_Ratio</th>\n",
       "      <th>FPR_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20–29</td>\n",
       "      <td>9083</td>\n",
       "      <td>0.580975</td>\n",
       "      <td>0.877437</td>\n",
       "      <td>0.541094</td>\n",
       "      <td>0.179079</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30–39</td>\n",
       "      <td>16277</td>\n",
       "      <td>0.403207</td>\n",
       "      <td>0.788500</td>\n",
       "      <td>0.363723</td>\n",
       "      <td>0.181777</td>\n",
       "      <td>0.898639</td>\n",
       "      <td>0.672198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40–49</td>\n",
       "      <td>15236</td>\n",
       "      <td>0.320688</td>\n",
       "      <td>0.733610</td>\n",
       "      <td>0.285226</td>\n",
       "      <td>0.180925</td>\n",
       "      <td>0.836082</td>\n",
       "      <td>0.527127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50–59</td>\n",
       "      <td>13624</td>\n",
       "      <td>0.271359</td>\n",
       "      <td>0.659314</td>\n",
       "      <td>0.246643</td>\n",
       "      <td>0.145523</td>\n",
       "      <td>0.751408</td>\n",
       "      <td>0.455822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60–69</td>\n",
       "      <td>7223</td>\n",
       "      <td>0.193271</td>\n",
       "      <td>0.507163</td>\n",
       "      <td>0.177335</td>\n",
       "      <td>0.126791</td>\n",
       "      <td>0.578005</td>\n",
       "      <td>0.327734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  AGE_GROUP      n  Pos Rate  TPR (Equal Opportunity)       FPR  \\\n",
       "0     20–29   9083  0.580975                 0.877437  0.541094   \n",
       "1     30–39  16277  0.403207                 0.788500  0.363723   \n",
       "2     40–49  15236  0.320688                 0.733610  0.285226   \n",
       "3     50–59  13624  0.271359                 0.659314  0.246643   \n",
       "4     60–69   7223  0.193271                 0.507163  0.177335   \n",
       "\n",
       "   Precision (PPVP)  TPR_Ratio  FPR_Ratio  \n",
       "0          0.179079   1.000000   1.000000  \n",
       "1          0.181777   0.898639   0.672198  \n",
       "2          0.180925   0.836082   0.527127  \n",
       "3          0.145523   0.751408   0.455822  \n",
       "4          0.126791   0.578005   0.327734  "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fairness_metrics(apps_holdout_strat, 'AGE_GROUP', 'TARGET', 'y_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MARITAL_STATUS</th>\n",
       "      <th>n</th>\n",
       "      <th>Pos Rate</th>\n",
       "      <th>TPR (Equal Opportunity)</th>\n",
       "      <th>FPR</th>\n",
       "      <th>Precision (PPVP)</th>\n",
       "      <th>TPR_Ratio</th>\n",
       "      <th>FPR_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Married</td>\n",
       "      <td>45224</td>\n",
       "      <td>0.349018</td>\n",
       "      <td>0.754053</td>\n",
       "      <td>0.314220</td>\n",
       "      <td>0.170933</td>\n",
       "      <td>0.955277</td>\n",
       "      <td>0.800637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Previously Married</td>\n",
       "      <td>7190</td>\n",
       "      <td>0.297775</td>\n",
       "      <td>0.685371</td>\n",
       "      <td>0.268869</td>\n",
       "      <td>0.159738</td>\n",
       "      <td>0.868267</td>\n",
       "      <td>0.685081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Single</td>\n",
       "      <td>9029</td>\n",
       "      <td>0.431277</td>\n",
       "      <td>0.789354</td>\n",
       "      <td>0.392463</td>\n",
       "      <td>0.178993</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       MARITAL_STATUS      n  Pos Rate  TPR (Equal Opportunity)       FPR  \\\n",
       "0             Married  45224  0.349018                 0.754053  0.314220   \n",
       "1  Previously Married   7190  0.297775                 0.685371  0.268869   \n",
       "2              Single   9029  0.431277                 0.789354  0.392463   \n",
       "\n",
       "   Precision (PPVP)  TPR_Ratio  FPR_Ratio  \n",
       "0          0.170933   0.955277   0.800637  \n",
       "1          0.159738   0.868267   0.685081  \n",
       "2          0.178993   1.000000   1.000000  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fairness_metrics(apps_holdout_strat, 'MARITAL_STATUS', 'TARGET', 'y_pred')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
