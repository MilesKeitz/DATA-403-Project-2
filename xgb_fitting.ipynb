{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning an XGBoost Model\n",
    "\n",
    "The goal of this notebook is to train and evaluate an XGBoost model, comparing it's performance on a holdout set against other types of models (LR, SVC, LDA). \n",
    "\n",
    "To ensure reproducibility and consistent evaluation across models, all datasets were **pre-split into cross-val data and holdout data** as below:\n",
    "\n",
    "| Split type           | CV training file     | Holdout file              | Description                              |\n",
    "| -------------------- | -------------------- | ------------------------- | ---------------------------------------- |\n",
    "| **Random**           | `apps_cv_random.csv` | `apps_holdout_random.csv` | Simple random sampling                   |\n",
    "| **Stratified**       | `apps_cv_strat.csv`  | `apps_holdout_strat.csv`  | Stratified by `TARGET`                   |\n",
    "| **Multi-Stratified** | `apps_cv_multi.csv`  | `apps_holdout_multi.csv`  | Stratified by `TARGET` + `CODE_GENDER_M` |\n",
    "\n",
    "Each dataset for cross-validation (`apps_cv_*.csv`) also contains a column, `fold`, with pre-assigned folds from 1-5 using the corresponding splitting method to ensure consistent evaluation. Therefore, no additional splitting is needed inside this notebook -- can simply loop through assigned folds for cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions\n",
    "\n",
    "Copying over metric functions from `cross_val.ipynb`. Note that in that file, we assigned folds already for each type of splitting to maintain consistent comparison across modeling. So, there will be no explicit splitting in this fild, we will just use the folds already created, but still make it clear what type of splitting was used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICS \n",
    "\n",
    "def classification_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    computes conf matrix + acc, prec, rec, and f1\n",
    "    \n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    # conf matrix\n",
    "    tp = np.sum((y_true==1) & (y_pred==1))\n",
    "    tn = np.sum((y_true==0) & (y_pred==0))\n",
    "    fp = np.sum((y_true==0) & (y_pred==1))\n",
    "    fn = np.sum((y_true==1) & (y_pred==0))\n",
    "\n",
    "    acc  = (tp + tn) / max((tp + tn + fp + fn), 1)\n",
    "    prec = tp / max((tp + fp), 1)\n",
    "    rec  = tp / max((tp + fn), 1)\n",
    "    f1   = (2*prec*rec / max((prec+rec), 1e-12)) if (prec+rec)>0 else 0.0\n",
    "\n",
    "    metrics = {\n",
    "        \"tp\":tp, \"tn\": tn, \"fp\":fp, \"fn\":fn, \"acc\":acc, \"prec\":prec, \"rec\": rec, \"f1\":f1\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def roc_auc_from_probs(y_true, y_prob):\n",
    "    \n",
    "    desc_sort_indices = np.argsort(-y_prob)\n",
    "    y_true = np.array(y_true)[desc_sort_indices]\n",
    "    y_prob = np.array(y_prob)[desc_sort_indices]\n",
    "    pos = np.sum(y_true == 1)\n",
    "    neg = np.sum(y_true == 0)\n",
    "\n",
    "    # running totals for TPR/FPR\n",
    "    tpr = [0.0]\n",
    "    fpr = [0.0]\n",
    "    tp = fp = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "        tpr.append(tp / pos)\n",
    "        fpr.append(fp / neg)\n",
    "\n",
    "    # get auc\n",
    "    auc = np.trapz(tpr, fpr)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development\n",
    "\n",
    "**Notes:** \n",
    "- All evaluation will focus on stratified cross-validation, but we will test the other methods as well. \n",
    "- Recall that folds have been pre-assigned to ensure consistency across different model development processes\n",
    "\n",
    "**Process:**\n",
    "1. Setting a baseline\n",
    "    - evaluating an xgb model with all default parameters\n",
    "2. Hyperparameter tuning\n",
    "    - evaluate many different combinations of parameters\n",
    "    - choose the best set based on average ROC-AUC across all folds\n",
    "3. Threshold tuning\n",
    "    - tweak the threshold on the best model to maximize F1 \n",
    "        - note that roc-auc is not affected by threshold, so we need a different optimizing metric\n",
    "4. Holdout evaluation\n",
    "    - evaluate on the corresponding holdout table. the performance here is what we will compare with other models (LR, SVC, LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting a Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
